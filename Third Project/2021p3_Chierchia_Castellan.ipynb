{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.7 64-bit ('project_venv': venv)"
    },
    "interpreter": {
      "hash": "4f66127a04849ecf4e9f60d7fd6e8bb0c43d5aebaba76dfbe36fa82666a8311c"
    },
    "colab": {
      "name": "unaNuovaSperanza_project_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28caf7aa9e404f44a50097aef2fe0e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e3f8da556ec4928a3baca6f95e0ccdd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b88652d5cbe44509b4ca2d2f5837a6f6",
              "IPY_MODEL_830929901a964b5287bb7fd756859d75",
              "IPY_MODEL_ba3a53f2a1e24ff6b0013d5fce3a8eb4"
            ]
          }
        },
        "1e3f8da556ec4928a3baca6f95e0ccdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b88652d5cbe44509b4ca2d2f5837a6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_34700e24b3b444bc83a943aaf4a4287d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b54242fa5264c1fbcfa912ed2b1232a"
          }
        },
        "830929901a964b5287bb7fd756859d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4812873bcdeb4e95ba856b2060d9776d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3987ec0e59d74ede95ad16047cc10ec1"
          }
        },
        "ba3a53f2a1e24ff6b0013d5fce3a8eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49442f30fdd3424e8abfa9cc6936bb87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:13&lt;00:00, 15355947.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_642c8b3aa2f646bea3a449c7ceb61059"
          }
        },
        "34700e24b3b444bc83a943aaf4a4287d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b54242fa5264c1fbcfa912ed2b1232a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4812873bcdeb4e95ba856b2060d9776d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3987ec0e59d74ede95ad16047cc10ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49442f30fdd3424e8abfa9cc6936bb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "642c8b3aa2f646bea3a449c7ceb61059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WkmgqRQmQjC"
      },
      "source": [
        "# Project 3 CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg6Tj_CZTXMg",
        "outputId": "cd52cfdd-38ff-4d45-da8e-301098f0326f"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 157 µs (started: 2021-10-15 14:20:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STjDt9hfmQjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd5df42-8f68-4d6b-a637-90dee3a0ac1a"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 25 s (started: 2021-10-15 14:20:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWIg0Jh6mQjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f91384-2701-4d8f-ea2b-554a63ba32f6"
      },
      "source": [
        "# CUDA \n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 74.4 ms (started: 2021-10-15 14:20:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94RaybxemQjW"
      },
      "source": [
        "## Cast and Normalize Pixel Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S1WB-1dmQjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e44a162-fb69-4177-d847-3dc520dfed6b"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[.5, .5, .5], std=[1 ,1, 1])\n",
        "transform_ = transforms.Compose([transforms.ToTensor(), normalize])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.36 ms (started: 2021-10-15 14:20:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "28caf7aa9e404f44a50097aef2fe0e60",
            "1e3f8da556ec4928a3baca6f95e0ccdd",
            "b88652d5cbe44509b4ca2d2f5837a6f6",
            "830929901a964b5287bb7fd756859d75",
            "ba3a53f2a1e24ff6b0013d5fce3a8eb4",
            "34700e24b3b444bc83a943aaf4a4287d",
            "3b54242fa5264c1fbcfa912ed2b1232a",
            "4812873bcdeb4e95ba856b2060d9776d",
            "3987ec0e59d74ede95ad16047cc10ec1",
            "49442f30fdd3424e8abfa9cc6936bb87",
            "642c8b3aa2f646bea3a449c7ceb61059"
          ]
        },
        "id": "pkb5vRdnmQjY",
        "outputId": "7a7e8d8b-f4c2-4d02-f2e4-9b5f46e1b629"
      },
      "source": [
        "#CIFAR-10 dataset.\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform_,\n",
        "                                             download=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28caf7aa9e404f44a50097aef2fe0e60",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n",
            "time: 18.2 s (started: 2021-10-15 14:20:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ4h3IL1mQjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9f1f4f-04e9-42d8-fe10-60655544516b"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 747 µs (started: 2021-10-15 14:20:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGgfwYXUmQjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c09b643-8e24-4049-c649-2b91e6efd416"
      },
      "source": [
        "# Parameters\n",
        "\n",
        "learning_rate = 0.001 # 0.001 or 0.1\n",
        "mini_batch = 64 # 64 or 256\n",
        "epochs = 300"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.46 ms (started: 2021-10-15 14:20:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWKxaCWrmQjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12049f2-6216-43fc-eb5a-93642013b2be"
      },
      "source": [
        "# Data loader \n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,   \n",
        "                                           batch_size=mini_batch,\n",
        "                                           shuffle=True) # True of False\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                          train=False,\n",
        "                                          download=True, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=mini_batch, \n",
        "                                          shuffle=False) # True of False\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "time: 831 ms (started: 2021-10-15 14:20:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KdocGZ7_KKR"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMGHRnhPmQjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f64dae7-21f6-4521-a4d0-0a545053a761"
      },
      "source": [
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=0),\n",
        "            #nn.Conv2d(3, 24, kernel_size=5, stride=1, padding=0),\n",
        "            #nn.Conv2d(3, 24, kernel_size=7, stride=1, padding=0),\n",
        "            #nn.BatchNorm2d(24),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),\n",
        "            #nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=0),\n",
        "            #nn.Conv2d(24, 48, kernel_size=5, stride=1, padding=0),\n",
        "            #nn.BatchNorm2d(48),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0),\n",
        "            #nn.Conv2d(48, 96, kernel_size=5, stride=1, padding=0),\n",
        "            #nn.BatchNorm2d(96),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "\n",
        "            nn.Linear(2*2*256, 512),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Linear(512,128),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, num_classes),\n",
        "\n",
        "            nn.Softmax())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 28.6 ms (started: 2021-10-15 17:24:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeQyY4SrmQji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66dc644-15ac-4622-95db-a133ee2f0582"
      },
      "source": [
        "classes = 10\n",
        "\n",
        "model = ConvNet(classes).to(device)\n",
        "summary(model, (3,32,32))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 28, 28]           4,864\n",
            "         LeakyReLU-2           [-1, 64, 28, 28]               0\n",
            "       BatchNorm2d-3           [-1, 64, 28, 28]             128\n",
            "         MaxPool2d-4           [-1, 64, 14, 14]               0\n",
            "            Conv2d-5          [-1, 128, 12, 12]          73,856\n",
            "         LeakyReLU-6          [-1, 128, 12, 12]               0\n",
            "       BatchNorm2d-7          [-1, 128, 12, 12]             256\n",
            "         MaxPool2d-8            [-1, 128, 6, 6]               0\n",
            "            Conv2d-9            [-1, 256, 4, 4]         295,168\n",
            "        MaxPool2d-10            [-1, 256, 2, 2]               0\n",
            "          Flatten-11                 [-1, 1024]               0\n",
            "           Linear-12                  [-1, 512]         524,800\n",
            "          Dropout-13                  [-1, 512]               0\n",
            "        LeakyReLU-14                  [-1, 512]               0\n",
            "      BatchNorm1d-15                  [-1, 512]           1,024\n",
            "           Linear-16                  [-1, 128]          65,664\n",
            "        LeakyReLU-17                  [-1, 128]               0\n",
            "      BatchNorm1d-18                  [-1, 128]             256\n",
            "           Linear-19                   [-1, 10]           1,290\n",
            "          Softmax-20                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 967,306\n",
            "Trainable params: 967,306\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.77\n",
            "Params size (MB): 3.69\n",
            "Estimated Total Size (MB): 5.47\n",
            "----------------------------------------------------------------\n",
            "time: 42.1 ms (started: 2021-10-15 17:24:10 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7uISUSYmQjj"
      },
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0g5a_lkmQjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4d5e67-3110-4603-c927-e551e85fd5be"
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) #if needed momentum=0.9"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.55 ms (started: 2021-10-15 17:24:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vquYSZv7mQjl",
        "outputId": "fdbe2697-f003-4dfd-8e46-df7899dc3518"
      },
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Reset before!!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Step [100/782], Loss: 2.2114\n",
            "Epoch [1/300], Step [200/782], Loss: 2.1865\n",
            "Epoch [1/300], Step [300/782], Loss: 2.1490\n",
            "Epoch [1/300], Step [400/782], Loss: 2.1165\n",
            "Epoch [1/300], Step [500/782], Loss: 2.0680\n",
            "Epoch [1/300], Step [600/782], Loss: 2.0646\n",
            "Epoch [1/300], Step [700/782], Loss: 2.0538\n",
            "Epoch [2/300], Step [100/782], Loss: 2.0506\n",
            "Epoch [2/300], Step [200/782], Loss: 1.9390\n",
            "Epoch [2/300], Step [300/782], Loss: 2.0063\n",
            "Epoch [2/300], Step [400/782], Loss: 2.0134\n",
            "Epoch [2/300], Step [500/782], Loss: 2.0052\n",
            "Epoch [2/300], Step [600/782], Loss: 1.9494\n",
            "Epoch [2/300], Step [700/782], Loss: 1.8860\n",
            "Epoch [3/300], Step [100/782], Loss: 1.9025\n",
            "Epoch [3/300], Step [200/782], Loss: 1.9171\n",
            "Epoch [3/300], Step [300/782], Loss: 1.8193\n",
            "Epoch [3/300], Step [400/782], Loss: 1.9158\n",
            "Epoch [3/300], Step [500/782], Loss: 1.9544\n",
            "Epoch [3/300], Step [600/782], Loss: 1.9883\n",
            "Epoch [3/300], Step [700/782], Loss: 1.9687\n",
            "Epoch [4/300], Step [100/782], Loss: 1.7922\n",
            "Epoch [4/300], Step [200/782], Loss: 1.9576\n",
            "Epoch [4/300], Step [300/782], Loss: 1.8112\n",
            "Epoch [4/300], Step [400/782], Loss: 1.8826\n",
            "Epoch [4/300], Step [500/782], Loss: 1.8046\n",
            "Epoch [4/300], Step [600/782], Loss: 1.8608\n",
            "Epoch [4/300], Step [700/782], Loss: 1.9610\n",
            "Epoch [5/300], Step [100/782], Loss: 1.8181\n",
            "Epoch [5/300], Step [200/782], Loss: 1.7862\n",
            "Epoch [5/300], Step [300/782], Loss: 1.8458\n",
            "Epoch [5/300], Step [400/782], Loss: 1.8258\n",
            "Epoch [5/300], Step [500/782], Loss: 1.8662\n",
            "Epoch [5/300], Step [600/782], Loss: 1.8488\n",
            "Epoch [5/300], Step [700/782], Loss: 1.9062\n",
            "Epoch [6/300], Step [100/782], Loss: 1.7999\n",
            "Epoch [6/300], Step [200/782], Loss: 1.8648\n",
            "Epoch [6/300], Step [300/782], Loss: 1.7389\n",
            "Epoch [6/300], Step [400/782], Loss: 1.7623\n",
            "Epoch [6/300], Step [500/782], Loss: 1.8379\n",
            "Epoch [6/300], Step [600/782], Loss: 1.8961\n",
            "Epoch [6/300], Step [700/782], Loss: 1.8972\n",
            "Epoch [7/300], Step [100/782], Loss: 1.6911\n",
            "Epoch [7/300], Step [200/782], Loss: 1.7930\n",
            "Epoch [7/300], Step [300/782], Loss: 1.7805\n",
            "Epoch [7/300], Step [400/782], Loss: 1.7329\n",
            "Epoch [7/300], Step [500/782], Loss: 1.8432\n",
            "Epoch [7/300], Step [600/782], Loss: 1.7671\n",
            "Epoch [7/300], Step [700/782], Loss: 1.7378\n",
            "Epoch [8/300], Step [100/782], Loss: 1.8137\n",
            "Epoch [8/300], Step [200/782], Loss: 1.8087\n",
            "Epoch [8/300], Step [300/782], Loss: 1.7964\n",
            "Epoch [8/300], Step [400/782], Loss: 1.8037\n",
            "Epoch [8/300], Step [500/782], Loss: 1.7245\n",
            "Epoch [8/300], Step [600/782], Loss: 1.8447\n",
            "Epoch [8/300], Step [700/782], Loss: 1.7693\n",
            "Epoch [9/300], Step [100/782], Loss: 1.8092\n",
            "Epoch [9/300], Step [200/782], Loss: 1.8459\n",
            "Epoch [9/300], Step [300/782], Loss: 1.7306\n",
            "Epoch [9/300], Step [400/782], Loss: 1.8355\n",
            "Epoch [9/300], Step [500/782], Loss: 1.7808\n",
            "Epoch [9/300], Step [600/782], Loss: 1.7256\n",
            "Epoch [9/300], Step [700/782], Loss: 1.7040\n",
            "Epoch [10/300], Step [100/782], Loss: 1.7183\n",
            "Epoch [10/300], Step [200/782], Loss: 1.7553\n",
            "Epoch [10/300], Step [300/782], Loss: 1.7189\n",
            "Epoch [10/300], Step [400/782], Loss: 1.7911\n",
            "Epoch [10/300], Step [500/782], Loss: 1.7542\n",
            "Epoch [10/300], Step [600/782], Loss: 1.7285\n",
            "Epoch [10/300], Step [700/782], Loss: 1.6683\n",
            "Epoch [11/300], Step [100/782], Loss: 1.8130\n",
            "Epoch [11/300], Step [200/782], Loss: 1.6251\n",
            "Epoch [11/300], Step [300/782], Loss: 1.7349\n",
            "Epoch [11/300], Step [400/782], Loss: 1.7422\n",
            "Epoch [11/300], Step [500/782], Loss: 1.7722\n",
            "Epoch [11/300], Step [600/782], Loss: 1.8281\n",
            "Epoch [11/300], Step [700/782], Loss: 1.6518\n",
            "Epoch [12/300], Step [100/782], Loss: 1.6261\n",
            "Epoch [12/300], Step [200/782], Loss: 1.6182\n",
            "Epoch [12/300], Step [300/782], Loss: 1.7846\n",
            "Epoch [12/300], Step [400/782], Loss: 1.7783\n",
            "Epoch [12/300], Step [500/782], Loss: 1.7052\n",
            "Epoch [12/300], Step [600/782], Loss: 1.7564\n",
            "Epoch [12/300], Step [700/782], Loss: 1.7668\n",
            "Epoch [13/300], Step [100/782], Loss: 1.5827\n",
            "Epoch [13/300], Step [200/782], Loss: 1.6842\n",
            "Epoch [13/300], Step [300/782], Loss: 1.6814\n",
            "Epoch [13/300], Step [400/782], Loss: 1.6676\n",
            "Epoch [13/300], Step [500/782], Loss: 1.6470\n",
            "Epoch [13/300], Step [600/782], Loss: 1.7076\n",
            "Epoch [13/300], Step [700/782], Loss: 1.7419\n",
            "Epoch [14/300], Step [100/782], Loss: 1.6614\n",
            "Epoch [14/300], Step [200/782], Loss: 1.6707\n",
            "Epoch [14/300], Step [300/782], Loss: 1.7677\n",
            "Epoch [14/300], Step [400/782], Loss: 1.6687\n",
            "Epoch [14/300], Step [500/782], Loss: 1.7011\n",
            "Epoch [14/300], Step [600/782], Loss: 1.6904\n",
            "Epoch [14/300], Step [700/782], Loss: 1.7312\n",
            "Epoch [15/300], Step [100/782], Loss: 1.6766\n",
            "Epoch [15/300], Step [200/782], Loss: 1.7655\n",
            "Epoch [15/300], Step [300/782], Loss: 1.7475\n",
            "Epoch [15/300], Step [400/782], Loss: 1.6528\n",
            "Epoch [15/300], Step [500/782], Loss: 1.6801\n",
            "Epoch [15/300], Step [600/782], Loss: 1.7149\n",
            "Epoch [15/300], Step [700/782], Loss: 1.6439\n",
            "Epoch [16/300], Step [100/782], Loss: 1.6922\n",
            "Epoch [16/300], Step [200/782], Loss: 1.6437\n",
            "Epoch [16/300], Step [300/782], Loss: 1.6101\n",
            "Epoch [16/300], Step [400/782], Loss: 1.6228\n",
            "Epoch [16/300], Step [500/782], Loss: 1.6755\n",
            "Epoch [16/300], Step [600/782], Loss: 1.7294\n",
            "Epoch [16/300], Step [700/782], Loss: 1.6089\n",
            "Epoch [17/300], Step [100/782], Loss: 1.7287\n",
            "Epoch [17/300], Step [200/782], Loss: 1.6598\n",
            "Epoch [17/300], Step [300/782], Loss: 1.6498\n",
            "Epoch [17/300], Step [400/782], Loss: 1.6829\n",
            "Epoch [17/300], Step [500/782], Loss: 1.6566\n",
            "Epoch [17/300], Step [600/782], Loss: 1.6034\n",
            "Epoch [17/300], Step [700/782], Loss: 1.6747\n",
            "Epoch [18/300], Step [100/782], Loss: 1.6113\n",
            "Epoch [18/300], Step [200/782], Loss: 1.6452\n",
            "Epoch [18/300], Step [300/782], Loss: 1.5853\n",
            "Epoch [18/300], Step [400/782], Loss: 1.7138\n",
            "Epoch [18/300], Step [500/782], Loss: 1.6391\n",
            "Epoch [18/300], Step [600/782], Loss: 1.6930\n",
            "Epoch [18/300], Step [700/782], Loss: 1.6778\n",
            "Epoch [19/300], Step [100/782], Loss: 1.6273\n",
            "Epoch [19/300], Step [200/782], Loss: 1.6128\n",
            "Epoch [19/300], Step [300/782], Loss: 1.6842\n",
            "Epoch [19/300], Step [400/782], Loss: 1.6041\n",
            "Epoch [19/300], Step [500/782], Loss: 1.6524\n",
            "Epoch [19/300], Step [600/782], Loss: 1.6119\n",
            "Epoch [19/300], Step [700/782], Loss: 1.6262\n",
            "Epoch [20/300], Step [100/782], Loss: 1.5213\n",
            "Epoch [20/300], Step [200/782], Loss: 1.6925\n",
            "Epoch [20/300], Step [300/782], Loss: 1.6716\n",
            "Epoch [20/300], Step [400/782], Loss: 1.6806\n",
            "Epoch [20/300], Step [500/782], Loss: 1.5831\n",
            "Epoch [20/300], Step [600/782], Loss: 1.5701\n",
            "Epoch [20/300], Step [700/782], Loss: 1.6714\n",
            "Epoch [21/300], Step [100/782], Loss: 1.5929\n",
            "Epoch [21/300], Step [200/782], Loss: 1.6377\n",
            "Epoch [21/300], Step [300/782], Loss: 1.5583\n",
            "Epoch [21/300], Step [400/782], Loss: 1.6316\n",
            "Epoch [21/300], Step [500/782], Loss: 1.6232\n",
            "Epoch [21/300], Step [600/782], Loss: 1.6343\n",
            "Epoch [21/300], Step [700/782], Loss: 1.5783\n",
            "Epoch [22/300], Step [100/782], Loss: 1.6060\n",
            "Epoch [22/300], Step [200/782], Loss: 1.5799\n",
            "Epoch [22/300], Step [300/782], Loss: 1.6809\n",
            "Epoch [22/300], Step [400/782], Loss: 1.5646\n",
            "Epoch [22/300], Step [500/782], Loss: 1.6195\n",
            "Epoch [22/300], Step [600/782], Loss: 1.6221\n",
            "Epoch [22/300], Step [700/782], Loss: 1.6191\n",
            "Epoch [23/300], Step [100/782], Loss: 1.5857\n",
            "Epoch [23/300], Step [200/782], Loss: 1.6340\n",
            "Epoch [23/300], Step [300/782], Loss: 1.6144\n",
            "Epoch [23/300], Step [400/782], Loss: 1.5407\n",
            "Epoch [23/300], Step [500/782], Loss: 1.6347\n",
            "Epoch [23/300], Step [600/782], Loss: 1.6285\n",
            "Epoch [23/300], Step [700/782], Loss: 1.5920\n",
            "Epoch [24/300], Step [100/782], Loss: 1.5876\n",
            "Epoch [24/300], Step [200/782], Loss: 1.6020\n",
            "Epoch [24/300], Step [300/782], Loss: 1.5380\n",
            "Epoch [24/300], Step [400/782], Loss: 1.6196\n",
            "Epoch [24/300], Step [500/782], Loss: 1.5481\n",
            "Epoch [24/300], Step [600/782], Loss: 1.5750\n",
            "Epoch [24/300], Step [700/782], Loss: 1.5993\n",
            "Epoch [25/300], Step [100/782], Loss: 1.5938\n",
            "Epoch [25/300], Step [200/782], Loss: 1.5751\n",
            "Epoch [25/300], Step [300/782], Loss: 1.5561\n",
            "Epoch [25/300], Step [400/782], Loss: 1.5923\n",
            "Epoch [25/300], Step [500/782], Loss: 1.6075\n",
            "Epoch [25/300], Step [600/782], Loss: 1.6280\n",
            "Epoch [25/300], Step [700/782], Loss: 1.5533\n",
            "Epoch [26/300], Step [100/782], Loss: 1.6365\n",
            "Epoch [26/300], Step [200/782], Loss: 1.5569\n",
            "Epoch [26/300], Step [300/782], Loss: 1.5500\n",
            "Epoch [26/300], Step [400/782], Loss: 1.5426\n",
            "Epoch [26/300], Step [500/782], Loss: 1.6619\n",
            "Epoch [26/300], Step [600/782], Loss: 1.5839\n",
            "Epoch [26/300], Step [700/782], Loss: 1.6012\n",
            "Epoch [27/300], Step [100/782], Loss: 1.5341\n",
            "Epoch [27/300], Step [200/782], Loss: 1.5732\n",
            "Epoch [27/300], Step [300/782], Loss: 1.5596\n",
            "Epoch [27/300], Step [400/782], Loss: 1.5373\n",
            "Epoch [27/300], Step [500/782], Loss: 1.6009\n",
            "Epoch [27/300], Step [600/782], Loss: 1.5956\n",
            "Epoch [27/300], Step [700/782], Loss: 1.5796\n",
            "Epoch [28/300], Step [100/782], Loss: 1.5688\n",
            "Epoch [28/300], Step [200/782], Loss: 1.5729\n",
            "Epoch [28/300], Step [300/782], Loss: 1.5896\n",
            "Epoch [28/300], Step [400/782], Loss: 1.5930\n",
            "Epoch [28/300], Step [500/782], Loss: 1.6556\n",
            "Epoch [28/300], Step [600/782], Loss: 1.5962\n",
            "Epoch [28/300], Step [700/782], Loss: 1.5364\n",
            "Epoch [29/300], Step [100/782], Loss: 1.5811\n",
            "Epoch [29/300], Step [200/782], Loss: 1.5893\n",
            "Epoch [29/300], Step [300/782], Loss: 1.5813\n",
            "Epoch [29/300], Step [400/782], Loss: 1.5467\n",
            "Epoch [29/300], Step [500/782], Loss: 1.5321\n",
            "Epoch [29/300], Step [600/782], Loss: 1.5658\n",
            "Epoch [29/300], Step [700/782], Loss: 1.5884\n",
            "Epoch [30/300], Step [100/782], Loss: 1.5982\n",
            "Epoch [30/300], Step [200/782], Loss: 1.5362\n",
            "Epoch [30/300], Step [300/782], Loss: 1.5327\n",
            "Epoch [30/300], Step [400/782], Loss: 1.5573\n",
            "Epoch [30/300], Step [500/782], Loss: 1.5911\n",
            "Epoch [30/300], Step [600/782], Loss: 1.6427\n",
            "Epoch [30/300], Step [700/782], Loss: 1.5690\n",
            "Epoch [31/300], Step [100/782], Loss: 1.6082\n",
            "Epoch [31/300], Step [200/782], Loss: 1.5567\n",
            "Epoch [31/300], Step [300/782], Loss: 1.5336\n",
            "Epoch [31/300], Step [400/782], Loss: 1.5391\n",
            "Epoch [31/300], Step [500/782], Loss: 1.4951\n",
            "Epoch [31/300], Step [600/782], Loss: 1.5697\n",
            "Epoch [31/300], Step [700/782], Loss: 1.6304\n",
            "Epoch [32/300], Step [100/782], Loss: 1.6081\n",
            "Epoch [32/300], Step [200/782], Loss: 1.5690\n",
            "Epoch [32/300], Step [300/782], Loss: 1.5638\n",
            "Epoch [32/300], Step [400/782], Loss: 1.5286\n",
            "Epoch [32/300], Step [500/782], Loss: 1.5371\n",
            "Epoch [32/300], Step [600/782], Loss: 1.5719\n",
            "Epoch [32/300], Step [700/782], Loss: 1.5561\n",
            "Epoch [33/300], Step [100/782], Loss: 1.5430\n",
            "Epoch [33/300], Step [200/782], Loss: 1.5916\n",
            "Epoch [33/300], Step [300/782], Loss: 1.5405\n",
            "Epoch [33/300], Step [400/782], Loss: 1.5499\n",
            "Epoch [33/300], Step [500/782], Loss: 1.5216\n",
            "Epoch [33/300], Step [600/782], Loss: 1.5426\n",
            "Epoch [33/300], Step [700/782], Loss: 1.5627\n",
            "Epoch [34/300], Step [100/782], Loss: 1.4967\n",
            "Epoch [34/300], Step [200/782], Loss: 1.5243\n",
            "Epoch [34/300], Step [300/782], Loss: 1.5757\n",
            "Epoch [34/300], Step [400/782], Loss: 1.6440\n",
            "Epoch [34/300], Step [500/782], Loss: 1.5112\n",
            "Epoch [34/300], Step [600/782], Loss: 1.5803\n",
            "Epoch [34/300], Step [700/782], Loss: 1.5241\n",
            "Epoch [35/300], Step [100/782], Loss: 1.5393\n",
            "Epoch [35/300], Step [200/782], Loss: 1.5595\n",
            "Epoch [35/300], Step [300/782], Loss: 1.5648\n",
            "Epoch [35/300], Step [400/782], Loss: 1.5424\n",
            "Epoch [35/300], Step [500/782], Loss: 1.6163\n",
            "Epoch [35/300], Step [600/782], Loss: 1.5857\n",
            "Epoch [35/300], Step [700/782], Loss: 1.5266\n",
            "Epoch [36/300], Step [100/782], Loss: 1.5237\n",
            "Epoch [36/300], Step [200/782], Loss: 1.5687\n",
            "Epoch [36/300], Step [300/782], Loss: 1.5596\n",
            "Epoch [36/300], Step [400/782], Loss: 1.5074\n",
            "Epoch [36/300], Step [500/782], Loss: 1.5358\n",
            "Epoch [36/300], Step [600/782], Loss: 1.5482\n",
            "Epoch [36/300], Step [700/782], Loss: 1.5365\n",
            "Epoch [37/300], Step [100/782], Loss: 1.5561\n",
            "Epoch [37/300], Step [200/782], Loss: 1.5614\n",
            "Epoch [37/300], Step [300/782], Loss: 1.5232\n",
            "Epoch [37/300], Step [400/782], Loss: 1.5792\n",
            "Epoch [37/300], Step [500/782], Loss: 1.4935\n",
            "Epoch [37/300], Step [600/782], Loss: 1.5499\n",
            "Epoch [37/300], Step [700/782], Loss: 1.5042\n",
            "Epoch [38/300], Step [100/782], Loss: 1.5582\n",
            "Epoch [38/300], Step [200/782], Loss: 1.4883\n",
            "Epoch [38/300], Step [300/782], Loss: 1.5281\n",
            "Epoch [38/300], Step [400/782], Loss: 1.5304\n",
            "Epoch [38/300], Step [500/782], Loss: 1.5589\n",
            "Epoch [38/300], Step [600/782], Loss: 1.5275\n",
            "Epoch [38/300], Step [700/782], Loss: 1.5278\n",
            "Epoch [39/300], Step [100/782], Loss: 1.4978\n",
            "Epoch [39/300], Step [200/782], Loss: 1.5598\n",
            "Epoch [39/300], Step [300/782], Loss: 1.5182\n",
            "Epoch [39/300], Step [400/782], Loss: 1.5304\n",
            "Epoch [39/300], Step [500/782], Loss: 1.5331\n",
            "Epoch [39/300], Step [600/782], Loss: 1.6234\n",
            "Epoch [39/300], Step [700/782], Loss: 1.5712\n",
            "Epoch [40/300], Step [100/782], Loss: 1.5333\n",
            "Epoch [40/300], Step [200/782], Loss: 1.4972\n",
            "Epoch [40/300], Step [300/782], Loss: 1.5301\n",
            "Epoch [40/300], Step [400/782], Loss: 1.5940\n",
            "Epoch [40/300], Step [500/782], Loss: 1.5106\n",
            "Epoch [40/300], Step [600/782], Loss: 1.5893\n",
            "Epoch [40/300], Step [700/782], Loss: 1.5218\n",
            "Epoch [41/300], Step [100/782], Loss: 1.5036\n",
            "Epoch [41/300], Step [200/782], Loss: 1.4730\n",
            "Epoch [41/300], Step [300/782], Loss: 1.5485\n",
            "Epoch [41/300], Step [400/782], Loss: 1.5156\n",
            "Epoch [41/300], Step [500/782], Loss: 1.5604\n",
            "Epoch [41/300], Step [600/782], Loss: 1.5303\n",
            "Epoch [41/300], Step [700/782], Loss: 1.5035\n",
            "Epoch [42/300], Step [100/782], Loss: 1.5185\n",
            "Epoch [42/300], Step [200/782], Loss: 1.5095\n",
            "Epoch [42/300], Step [300/782], Loss: 1.5255\n",
            "Epoch [42/300], Step [400/782], Loss: 1.5034\n",
            "Epoch [42/300], Step [500/782], Loss: 1.5402\n",
            "Epoch [42/300], Step [600/782], Loss: 1.5071\n",
            "Epoch [42/300], Step [700/782], Loss: 1.5382\n",
            "Epoch [43/300], Step [100/782], Loss: 1.5467\n",
            "Epoch [43/300], Step [200/782], Loss: 1.5536\n",
            "Epoch [43/300], Step [300/782], Loss: 1.5704\n",
            "Epoch [43/300], Step [400/782], Loss: 1.5128\n",
            "Epoch [43/300], Step [500/782], Loss: 1.5405\n",
            "Epoch [43/300], Step [600/782], Loss: 1.5136\n",
            "Epoch [43/300], Step [700/782], Loss: 1.4974\n",
            "Epoch [44/300], Step [100/782], Loss: 1.5051\n",
            "Epoch [44/300], Step [200/782], Loss: 1.5165\n",
            "Epoch [44/300], Step [300/782], Loss: 1.5274\n",
            "Epoch [44/300], Step [400/782], Loss: 1.5265\n",
            "Epoch [44/300], Step [500/782], Loss: 1.6250\n",
            "Epoch [44/300], Step [600/782], Loss: 1.5025\n",
            "Epoch [44/300], Step [700/782], Loss: 1.5014\n",
            "Epoch [45/300], Step [100/782], Loss: 1.5191\n",
            "Epoch [45/300], Step [200/782], Loss: 1.5017\n",
            "Epoch [45/300], Step [300/782], Loss: 1.5178\n",
            "Epoch [45/300], Step [400/782], Loss: 1.5616\n",
            "Epoch [45/300], Step [500/782], Loss: 1.5155\n",
            "Epoch [45/300], Step [600/782], Loss: 1.5211\n",
            "Epoch [45/300], Step [700/782], Loss: 1.5436\n",
            "Epoch [46/300], Step [100/782], Loss: 1.4849\n",
            "Epoch [46/300], Step [200/782], Loss: 1.5511\n",
            "Epoch [46/300], Step [300/782], Loss: 1.4858\n",
            "Epoch [46/300], Step [400/782], Loss: 1.5433\n",
            "Epoch [46/300], Step [500/782], Loss: 1.5430\n",
            "Epoch [46/300], Step [600/782], Loss: 1.5107\n",
            "Epoch [46/300], Step [700/782], Loss: 1.5087\n",
            "Epoch [47/300], Step [100/782], Loss: 1.5327\n",
            "Epoch [47/300], Step [200/782], Loss: 1.5846\n",
            "Epoch [47/300], Step [300/782], Loss: 1.5084\n",
            "Epoch [47/300], Step [400/782], Loss: 1.5948\n",
            "Epoch [47/300], Step [500/782], Loss: 1.5762\n",
            "Epoch [47/300], Step [600/782], Loss: 1.5625\n",
            "Epoch [47/300], Step [700/782], Loss: 1.4942\n",
            "Epoch [48/300], Step [100/782], Loss: 1.5214\n",
            "Epoch [48/300], Step [200/782], Loss: 1.5272\n",
            "Epoch [48/300], Step [300/782], Loss: 1.4908\n",
            "Epoch [48/300], Step [400/782], Loss: 1.5000\n",
            "Epoch [48/300], Step [500/782], Loss: 1.5530\n",
            "Epoch [48/300], Step [600/782], Loss: 1.5182\n",
            "Epoch [48/300], Step [700/782], Loss: 1.4952\n",
            "Epoch [49/300], Step [100/782], Loss: 1.5287\n",
            "Epoch [49/300], Step [200/782], Loss: 1.5605\n",
            "Epoch [49/300], Step [300/782], Loss: 1.5451\n",
            "Epoch [49/300], Step [400/782], Loss: 1.4817\n",
            "Epoch [49/300], Step [500/782], Loss: 1.5176\n",
            "Epoch [49/300], Step [600/782], Loss: 1.5142\n",
            "Epoch [49/300], Step [700/782], Loss: 1.4701\n",
            "Epoch [50/300], Step [100/782], Loss: 1.5183\n",
            "Epoch [50/300], Step [200/782], Loss: 1.5343\n",
            "Epoch [50/300], Step [300/782], Loss: 1.5056\n",
            "Epoch [50/300], Step [400/782], Loss: 1.5981\n",
            "Epoch [50/300], Step [500/782], Loss: 1.5307\n",
            "Epoch [50/300], Step [600/782], Loss: 1.5194\n",
            "Epoch [50/300], Step [700/782], Loss: 1.5124\n",
            "Epoch [51/300], Step [100/782], Loss: 1.5380\n",
            "Epoch [51/300], Step [200/782], Loss: 1.5375\n",
            "Epoch [51/300], Step [300/782], Loss: 1.5109\n",
            "Epoch [51/300], Step [400/782], Loss: 1.5247\n",
            "Epoch [51/300], Step [500/782], Loss: 1.4880\n",
            "Epoch [51/300], Step [600/782], Loss: 1.4857\n",
            "Epoch [51/300], Step [700/782], Loss: 1.5295\n",
            "Epoch [52/300], Step [100/782], Loss: 1.5343\n",
            "Epoch [52/300], Step [200/782], Loss: 1.5162\n",
            "Epoch [52/300], Step [300/782], Loss: 1.5263\n",
            "Epoch [52/300], Step [400/782], Loss: 1.5487\n",
            "Epoch [52/300], Step [500/782], Loss: 1.5077\n",
            "Epoch [52/300], Step [600/782], Loss: 1.4992\n",
            "Epoch [52/300], Step [700/782], Loss: 1.5886\n",
            "Epoch [53/300], Step [100/782], Loss: 1.5042\n",
            "Epoch [53/300], Step [200/782], Loss: 1.4915\n",
            "Epoch [53/300], Step [300/782], Loss: 1.5054\n",
            "Epoch [53/300], Step [400/782], Loss: 1.4921\n",
            "Epoch [53/300], Step [500/782], Loss: 1.4670\n",
            "Epoch [53/300], Step [600/782], Loss: 1.5348\n",
            "Epoch [53/300], Step [700/782], Loss: 1.5311\n",
            "Epoch [54/300], Step [100/782], Loss: 1.5169\n",
            "Epoch [54/300], Step [200/782], Loss: 1.4977\n",
            "Epoch [54/300], Step [300/782], Loss: 1.5364\n",
            "Epoch [54/300], Step [400/782], Loss: 1.4801\n",
            "Epoch [54/300], Step [500/782], Loss: 1.5114\n",
            "Epoch [54/300], Step [600/782], Loss: 1.5158\n",
            "Epoch [54/300], Step [700/782], Loss: 1.4929\n",
            "Epoch [55/300], Step [100/782], Loss: 1.5741\n",
            "Epoch [55/300], Step [200/782], Loss: 1.4789\n",
            "Epoch [55/300], Step [300/782], Loss: 1.4946\n",
            "Epoch [55/300], Step [400/782], Loss: 1.5312\n",
            "Epoch [55/300], Step [500/782], Loss: 1.5924\n",
            "Epoch [55/300], Step [600/782], Loss: 1.5070\n",
            "Epoch [55/300], Step [700/782], Loss: 1.4912\n",
            "Epoch [56/300], Step [100/782], Loss: 1.4801\n",
            "Epoch [56/300], Step [200/782], Loss: 1.5306\n",
            "Epoch [56/300], Step [300/782], Loss: 1.4969\n",
            "Epoch [56/300], Step [400/782], Loss: 1.5218\n",
            "Epoch [56/300], Step [500/782], Loss: 1.5104\n",
            "Epoch [56/300], Step [600/782], Loss: 1.4859\n",
            "Epoch [56/300], Step [700/782], Loss: 1.5330\n",
            "Epoch [57/300], Step [100/782], Loss: 1.4975\n",
            "Epoch [57/300], Step [200/782], Loss: 1.4965\n",
            "Epoch [57/300], Step [300/782], Loss: 1.5398\n",
            "Epoch [57/300], Step [400/782], Loss: 1.5351\n",
            "Epoch [57/300], Step [500/782], Loss: 1.5195\n",
            "Epoch [57/300], Step [600/782], Loss: 1.4771\n",
            "Epoch [57/300], Step [700/782], Loss: 1.5257\n",
            "Epoch [58/300], Step [100/782], Loss: 1.4830\n",
            "Epoch [58/300], Step [200/782], Loss: 1.5022\n",
            "Epoch [58/300], Step [300/782], Loss: 1.5131\n",
            "Epoch [58/300], Step [400/782], Loss: 1.4933\n",
            "Epoch [58/300], Step [500/782], Loss: 1.4984\n",
            "Epoch [58/300], Step [600/782], Loss: 1.4772\n",
            "Epoch [58/300], Step [700/782], Loss: 1.4960\n",
            "Epoch [59/300], Step [100/782], Loss: 1.4850\n",
            "Epoch [59/300], Step [200/782], Loss: 1.5002\n",
            "Epoch [59/300], Step [300/782], Loss: 1.4921\n",
            "Epoch [59/300], Step [400/782], Loss: 1.5117\n",
            "Epoch [59/300], Step [500/782], Loss: 1.5228\n",
            "Epoch [59/300], Step [600/782], Loss: 1.4860\n",
            "Epoch [59/300], Step [700/782], Loss: 1.5400\n",
            "Epoch [60/300], Step [100/782], Loss: 1.5133\n",
            "Epoch [60/300], Step [200/782], Loss: 1.4854\n",
            "Epoch [60/300], Step [300/782], Loss: 1.5243\n",
            "Epoch [60/300], Step [400/782], Loss: 1.5162\n",
            "Epoch [60/300], Step [500/782], Loss: 1.5142\n",
            "Epoch [60/300], Step [600/782], Loss: 1.5335\n",
            "Epoch [60/300], Step [700/782], Loss: 1.4950\n",
            "Epoch [61/300], Step [100/782], Loss: 1.5320\n",
            "Epoch [61/300], Step [200/782], Loss: 1.5339\n",
            "Epoch [61/300], Step [300/782], Loss: 1.5097\n",
            "Epoch [61/300], Step [400/782], Loss: 1.5165\n",
            "Epoch [61/300], Step [500/782], Loss: 1.4768\n",
            "Epoch [61/300], Step [600/782], Loss: 1.4696\n",
            "Epoch [61/300], Step [700/782], Loss: 1.5296\n",
            "Epoch [62/300], Step [100/782], Loss: 1.5017\n",
            "Epoch [62/300], Step [200/782], Loss: 1.5278\n",
            "Epoch [62/300], Step [300/782], Loss: 1.5319\n",
            "Epoch [62/300], Step [400/782], Loss: 1.4768\n",
            "Epoch [62/300], Step [500/782], Loss: 1.4917\n",
            "Epoch [62/300], Step [600/782], Loss: 1.4790\n",
            "Epoch [62/300], Step [700/782], Loss: 1.4813\n",
            "Epoch [63/300], Step [100/782], Loss: 1.4899\n",
            "Epoch [63/300], Step [200/782], Loss: 1.5095\n",
            "Epoch [63/300], Step [300/782], Loss: 1.5021\n",
            "Epoch [63/300], Step [400/782], Loss: 1.4965\n",
            "Epoch [63/300], Step [500/782], Loss: 1.4635\n",
            "Epoch [63/300], Step [600/782], Loss: 1.5176\n",
            "Epoch [63/300], Step [700/782], Loss: 1.5187\n",
            "Epoch [64/300], Step [100/782], Loss: 1.5329\n",
            "Epoch [64/300], Step [200/782], Loss: 1.5063\n",
            "Epoch [64/300], Step [300/782], Loss: 1.5039\n",
            "Epoch [64/300], Step [400/782], Loss: 1.5042\n",
            "Epoch [64/300], Step [500/782], Loss: 1.4680\n",
            "Epoch [64/300], Step [600/782], Loss: 1.4782\n",
            "Epoch [64/300], Step [700/782], Loss: 1.4786\n",
            "Epoch [65/300], Step [100/782], Loss: 1.4893\n",
            "Epoch [65/300], Step [200/782], Loss: 1.4696\n",
            "Epoch [65/300], Step [300/782], Loss: 1.5058\n",
            "Epoch [65/300], Step [400/782], Loss: 1.4976\n",
            "Epoch [65/300], Step [500/782], Loss: 1.4828\n",
            "Epoch [65/300], Step [600/782], Loss: 1.5122\n",
            "Epoch [65/300], Step [700/782], Loss: 1.5036\n",
            "Epoch [66/300], Step [100/782], Loss: 1.5131\n",
            "Epoch [66/300], Step [200/782], Loss: 1.4993\n",
            "Epoch [66/300], Step [300/782], Loss: 1.5055\n",
            "Epoch [66/300], Step [400/782], Loss: 1.4956\n",
            "Epoch [66/300], Step [500/782], Loss: 1.4984\n",
            "Epoch [66/300], Step [600/782], Loss: 1.5178\n",
            "Epoch [66/300], Step [700/782], Loss: 1.4848\n",
            "Epoch [67/300], Step [100/782], Loss: 1.4779\n",
            "Epoch [67/300], Step [200/782], Loss: 1.5519\n",
            "Epoch [67/300], Step [300/782], Loss: 1.4861\n",
            "Epoch [67/300], Step [400/782], Loss: 1.4800\n",
            "Epoch [67/300], Step [500/782], Loss: 1.5360\n",
            "Epoch [67/300], Step [600/782], Loss: 1.4933\n",
            "Epoch [67/300], Step [700/782], Loss: 1.5105\n",
            "Epoch [68/300], Step [100/782], Loss: 1.4850\n",
            "Epoch [68/300], Step [200/782], Loss: 1.4847\n",
            "Epoch [68/300], Step [300/782], Loss: 1.4930\n",
            "Epoch [68/300], Step [400/782], Loss: 1.5007\n",
            "Epoch [68/300], Step [500/782], Loss: 1.5175\n",
            "Epoch [68/300], Step [600/782], Loss: 1.4815\n",
            "Epoch [68/300], Step [700/782], Loss: 1.4649\n",
            "Epoch [69/300], Step [100/782], Loss: 1.4765\n",
            "Epoch [69/300], Step [200/782], Loss: 1.5118\n",
            "Epoch [69/300], Step [300/782], Loss: 1.4623\n",
            "Epoch [69/300], Step [400/782], Loss: 1.4760\n",
            "Epoch [69/300], Step [500/782], Loss: 1.4955\n",
            "Epoch [69/300], Step [600/782], Loss: 1.4926\n",
            "Epoch [69/300], Step [700/782], Loss: 1.5258\n",
            "Epoch [70/300], Step [100/782], Loss: 1.4865\n",
            "Epoch [70/300], Step [200/782], Loss: 1.5179\n",
            "Epoch [70/300], Step [300/782], Loss: 1.5360\n",
            "Epoch [70/300], Step [400/782], Loss: 1.4769\n",
            "Epoch [70/300], Step [500/782], Loss: 1.5227\n",
            "Epoch [70/300], Step [600/782], Loss: 1.4853\n",
            "Epoch [70/300], Step [700/782], Loss: 1.5413\n",
            "Epoch [71/300], Step [100/782], Loss: 1.4697\n",
            "Epoch [71/300], Step [200/782], Loss: 1.5128\n",
            "Epoch [71/300], Step [300/782], Loss: 1.4749\n",
            "Epoch [71/300], Step [400/782], Loss: 1.5079\n",
            "Epoch [71/300], Step [500/782], Loss: 1.5032\n",
            "Epoch [71/300], Step [600/782], Loss: 1.5266\n",
            "Epoch [71/300], Step [700/782], Loss: 1.4654\n",
            "Epoch [72/300], Step [100/782], Loss: 1.5076\n",
            "Epoch [72/300], Step [200/782], Loss: 1.5035\n",
            "Epoch [72/300], Step [300/782], Loss: 1.4969\n",
            "Epoch [72/300], Step [400/782], Loss: 1.5079\n",
            "Epoch [72/300], Step [500/782], Loss: 1.5121\n",
            "Epoch [72/300], Step [600/782], Loss: 1.4781\n",
            "Epoch [72/300], Step [700/782], Loss: 1.4986\n",
            "Epoch [73/300], Step [100/782], Loss: 1.4930\n",
            "Epoch [73/300], Step [200/782], Loss: 1.4657\n",
            "Epoch [73/300], Step [300/782], Loss: 1.4858\n",
            "Epoch [73/300], Step [400/782], Loss: 1.4974\n",
            "Epoch [73/300], Step [500/782], Loss: 1.4964\n",
            "Epoch [73/300], Step [600/782], Loss: 1.4658\n",
            "Epoch [73/300], Step [700/782], Loss: 1.4949\n",
            "Epoch [74/300], Step [100/782], Loss: 1.4792\n",
            "Epoch [74/300], Step [200/782], Loss: 1.4803\n",
            "Epoch [74/300], Step [300/782], Loss: 1.4943\n",
            "Epoch [74/300], Step [400/782], Loss: 1.4957\n",
            "Epoch [74/300], Step [500/782], Loss: 1.5040\n",
            "Epoch [74/300], Step [600/782], Loss: 1.4648\n",
            "Epoch [74/300], Step [700/782], Loss: 1.5341\n",
            "Epoch [75/300], Step [100/782], Loss: 1.4702\n",
            "Epoch [75/300], Step [200/782], Loss: 1.4808\n",
            "Epoch [75/300], Step [300/782], Loss: 1.4932\n",
            "Epoch [75/300], Step [400/782], Loss: 1.4968\n",
            "Epoch [75/300], Step [500/782], Loss: 1.5090\n",
            "Epoch [75/300], Step [600/782], Loss: 1.4935\n",
            "Epoch [75/300], Step [700/782], Loss: 1.5248\n",
            "Epoch [76/300], Step [100/782], Loss: 1.4929\n",
            "Epoch [76/300], Step [200/782], Loss: 1.4933\n",
            "Epoch [76/300], Step [300/782], Loss: 1.5060\n",
            "Epoch [76/300], Step [400/782], Loss: 1.4879\n",
            "Epoch [76/300], Step [500/782], Loss: 1.4837\n",
            "Epoch [76/300], Step [600/782], Loss: 1.4928\n",
            "Epoch [76/300], Step [700/782], Loss: 1.4640\n",
            "Epoch [77/300], Step [100/782], Loss: 1.4624\n",
            "Epoch [77/300], Step [200/782], Loss: 1.4808\n",
            "Epoch [77/300], Step [300/782], Loss: 1.5301\n",
            "Epoch [77/300], Step [400/782], Loss: 1.4624\n",
            "Epoch [77/300], Step [500/782], Loss: 1.4643\n",
            "Epoch [77/300], Step [600/782], Loss: 1.4794\n",
            "Epoch [77/300], Step [700/782], Loss: 1.4887\n",
            "Epoch [78/300], Step [100/782], Loss: 1.4787\n",
            "Epoch [78/300], Step [200/782], Loss: 1.5102\n",
            "Epoch [78/300], Step [300/782], Loss: 1.5155\n",
            "Epoch [78/300], Step [400/782], Loss: 1.5273\n",
            "Epoch [78/300], Step [500/782], Loss: 1.5027\n",
            "Epoch [78/300], Step [600/782], Loss: 1.4832\n",
            "Epoch [78/300], Step [700/782], Loss: 1.4774\n",
            "Epoch [79/300], Step [100/782], Loss: 1.4983\n",
            "Epoch [79/300], Step [200/782], Loss: 1.4882\n",
            "Epoch [79/300], Step [300/782], Loss: 1.4829\n",
            "Epoch [79/300], Step [400/782], Loss: 1.5083\n",
            "Epoch [79/300], Step [500/782], Loss: 1.4927\n",
            "Epoch [79/300], Step [600/782], Loss: 1.5036\n",
            "Epoch [79/300], Step [700/782], Loss: 1.4867\n",
            "Epoch [80/300], Step [100/782], Loss: 1.5323\n",
            "Epoch [80/300], Step [200/782], Loss: 1.5249\n",
            "Epoch [80/300], Step [300/782], Loss: 1.4655\n",
            "Epoch [80/300], Step [400/782], Loss: 1.5120\n",
            "Epoch [80/300], Step [500/782], Loss: 1.5169\n",
            "Epoch [80/300], Step [600/782], Loss: 1.5238\n",
            "Epoch [80/300], Step [700/782], Loss: 1.4870\n",
            "Epoch [81/300], Step [100/782], Loss: 1.5072\n",
            "Epoch [81/300], Step [200/782], Loss: 1.4749\n",
            "Epoch [81/300], Step [300/782], Loss: 1.4631\n",
            "Epoch [81/300], Step [400/782], Loss: 1.4621\n",
            "Epoch [81/300], Step [500/782], Loss: 1.5010\n",
            "Epoch [81/300], Step [600/782], Loss: 1.5066\n",
            "Epoch [81/300], Step [700/782], Loss: 1.5238\n",
            "Epoch [82/300], Step [100/782], Loss: 1.5127\n",
            "Epoch [82/300], Step [200/782], Loss: 1.5075\n",
            "Epoch [82/300], Step [300/782], Loss: 1.4629\n",
            "Epoch [82/300], Step [400/782], Loss: 1.4858\n",
            "Epoch [82/300], Step [500/782], Loss: 1.4776\n",
            "Epoch [82/300], Step [600/782], Loss: 1.5180\n",
            "Epoch [82/300], Step [700/782], Loss: 1.4626\n",
            "Epoch [83/300], Step [100/782], Loss: 1.5345\n",
            "Epoch [83/300], Step [200/782], Loss: 1.4941\n",
            "Epoch [83/300], Step [300/782], Loss: 1.4811\n",
            "Epoch [83/300], Step [400/782], Loss: 1.4932\n",
            "Epoch [83/300], Step [500/782], Loss: 1.4662\n",
            "Epoch [83/300], Step [600/782], Loss: 1.5170\n",
            "Epoch [83/300], Step [700/782], Loss: 1.4709\n",
            "Epoch [84/300], Step [100/782], Loss: 1.4962\n",
            "Epoch [84/300], Step [200/782], Loss: 1.4702\n",
            "Epoch [84/300], Step [300/782], Loss: 1.4776\n",
            "Epoch [84/300], Step [400/782], Loss: 1.5055\n",
            "Epoch [84/300], Step [500/782], Loss: 1.4818\n",
            "Epoch [84/300], Step [600/782], Loss: 1.4835\n",
            "Epoch [84/300], Step [700/782], Loss: 1.5404\n",
            "Epoch [85/300], Step [100/782], Loss: 1.4974\n",
            "Epoch [85/300], Step [200/782], Loss: 1.4951\n",
            "Epoch [85/300], Step [300/782], Loss: 1.5133\n",
            "Epoch [85/300], Step [400/782], Loss: 1.4791\n",
            "Epoch [85/300], Step [500/782], Loss: 1.5327\n",
            "Epoch [85/300], Step [600/782], Loss: 1.5091\n",
            "Epoch [85/300], Step [700/782], Loss: 1.4939\n",
            "Epoch [86/300], Step [100/782], Loss: 1.5398\n",
            "Epoch [86/300], Step [200/782], Loss: 1.4928\n",
            "Epoch [86/300], Step [300/782], Loss: 1.4619\n",
            "Epoch [86/300], Step [400/782], Loss: 1.4913\n",
            "Epoch [86/300], Step [500/782], Loss: 1.5245\n",
            "Epoch [86/300], Step [600/782], Loss: 1.4665\n",
            "Epoch [86/300], Step [700/782], Loss: 1.4784\n",
            "Epoch [87/300], Step [100/782], Loss: 1.4823\n",
            "Epoch [87/300], Step [200/782], Loss: 1.4621\n",
            "Epoch [87/300], Step [300/782], Loss: 1.5076\n",
            "Epoch [87/300], Step [400/782], Loss: 1.5084\n",
            "Epoch [87/300], Step [500/782], Loss: 1.4808\n",
            "Epoch [87/300], Step [600/782], Loss: 1.4783\n",
            "Epoch [87/300], Step [700/782], Loss: 1.4781\n",
            "Epoch [88/300], Step [100/782], Loss: 1.4624\n",
            "Epoch [88/300], Step [200/782], Loss: 1.4777\n",
            "Epoch [88/300], Step [300/782], Loss: 1.4655\n",
            "Epoch [88/300], Step [400/782], Loss: 1.4957\n",
            "Epoch [88/300], Step [500/782], Loss: 1.4986\n",
            "Epoch [88/300], Step [600/782], Loss: 1.4906\n",
            "Epoch [88/300], Step [700/782], Loss: 1.4945\n",
            "Epoch [89/300], Step [100/782], Loss: 1.4747\n",
            "Epoch [89/300], Step [200/782], Loss: 1.4994\n",
            "Epoch [89/300], Step [300/782], Loss: 1.5057\n",
            "Epoch [89/300], Step [400/782], Loss: 1.5149\n",
            "Epoch [89/300], Step [500/782], Loss: 1.4615\n",
            "Epoch [89/300], Step [600/782], Loss: 1.4846\n",
            "Epoch [89/300], Step [700/782], Loss: 1.4803\n",
            "Epoch [90/300], Step [100/782], Loss: 1.4636\n",
            "Epoch [90/300], Step [200/782], Loss: 1.5021\n",
            "Epoch [90/300], Step [300/782], Loss: 1.4731\n",
            "Epoch [90/300], Step [400/782], Loss: 1.4710\n",
            "Epoch [90/300], Step [500/782], Loss: 1.4778\n",
            "Epoch [90/300], Step [600/782], Loss: 1.4867\n",
            "Epoch [90/300], Step [700/782], Loss: 1.4779\n",
            "Epoch [91/300], Step [100/782], Loss: 1.4632\n",
            "Epoch [91/300], Step [200/782], Loss: 1.4846\n",
            "Epoch [91/300], Step [300/782], Loss: 1.4947\n",
            "Epoch [91/300], Step [400/782], Loss: 1.5117\n",
            "Epoch [91/300], Step [500/782], Loss: 1.4771\n",
            "Epoch [91/300], Step [600/782], Loss: 1.4640\n",
            "Epoch [91/300], Step [700/782], Loss: 1.4788\n",
            "Epoch [92/300], Step [100/782], Loss: 1.4933\n",
            "Epoch [92/300], Step [200/782], Loss: 1.4819\n",
            "Epoch [92/300], Step [300/782], Loss: 1.5264\n",
            "Epoch [92/300], Step [400/782], Loss: 1.4618\n",
            "Epoch [92/300], Step [500/782], Loss: 1.4686\n",
            "Epoch [92/300], Step [600/782], Loss: 1.4628\n",
            "Epoch [92/300], Step [700/782], Loss: 1.4778\n",
            "Epoch [93/300], Step [100/782], Loss: 1.4758\n",
            "Epoch [93/300], Step [200/782], Loss: 1.4767\n",
            "Epoch [93/300], Step [300/782], Loss: 1.4928\n",
            "Epoch [93/300], Step [400/782], Loss: 1.4788\n",
            "Epoch [93/300], Step [500/782], Loss: 1.5054\n",
            "Epoch [93/300], Step [600/782], Loss: 1.4794\n",
            "Epoch [93/300], Step [700/782], Loss: 1.4867\n",
            "Epoch [94/300], Step [100/782], Loss: 1.5235\n",
            "Epoch [94/300], Step [200/782], Loss: 1.5086\n",
            "Epoch [94/300], Step [300/782], Loss: 1.4783\n",
            "Epoch [94/300], Step [400/782], Loss: 1.4931\n",
            "Epoch [94/300], Step [500/782], Loss: 1.4618\n",
            "Epoch [94/300], Step [600/782], Loss: 1.4728\n",
            "Epoch [94/300], Step [700/782], Loss: 1.4926\n",
            "Epoch [95/300], Step [100/782], Loss: 1.4948\n",
            "Epoch [95/300], Step [200/782], Loss: 1.4933\n",
            "Epoch [95/300], Step [300/782], Loss: 1.4799\n",
            "Epoch [95/300], Step [400/782], Loss: 1.4850\n",
            "Epoch [95/300], Step [500/782], Loss: 1.4964\n",
            "Epoch [95/300], Step [600/782], Loss: 1.4982\n",
            "Epoch [95/300], Step [700/782], Loss: 1.4888\n",
            "Epoch [96/300], Step [100/782], Loss: 1.4770\n",
            "Epoch [96/300], Step [200/782], Loss: 1.5083\n",
            "Epoch [96/300], Step [300/782], Loss: 1.4936\n",
            "Epoch [96/300], Step [400/782], Loss: 1.4791\n",
            "Epoch [96/300], Step [500/782], Loss: 1.4844\n",
            "Epoch [96/300], Step [600/782], Loss: 1.5252\n",
            "Epoch [96/300], Step [700/782], Loss: 1.4771\n",
            "Epoch [97/300], Step [100/782], Loss: 1.4656\n",
            "Epoch [97/300], Step [200/782], Loss: 1.4617\n",
            "Epoch [97/300], Step [300/782], Loss: 1.4641\n",
            "Epoch [97/300], Step [400/782], Loss: 1.4772\n",
            "Epoch [97/300], Step [500/782], Loss: 1.4991\n",
            "Epoch [97/300], Step [600/782], Loss: 1.4641\n",
            "Epoch [97/300], Step [700/782], Loss: 1.4940\n",
            "Epoch [98/300], Step [100/782], Loss: 1.4768\n",
            "Epoch [98/300], Step [200/782], Loss: 1.4638\n",
            "Epoch [98/300], Step [300/782], Loss: 1.4787\n",
            "Epoch [98/300], Step [400/782], Loss: 1.4816\n",
            "Epoch [98/300], Step [500/782], Loss: 1.5072\n",
            "Epoch [98/300], Step [600/782], Loss: 1.5021\n",
            "Epoch [98/300], Step [700/782], Loss: 1.4794\n",
            "Epoch [99/300], Step [100/782], Loss: 1.4615\n",
            "Epoch [99/300], Step [200/782], Loss: 1.5384\n",
            "Epoch [99/300], Step [300/782], Loss: 1.4786\n",
            "Epoch [99/300], Step [400/782], Loss: 1.4944\n",
            "Epoch [99/300], Step [500/782], Loss: 1.4936\n",
            "Epoch [99/300], Step [600/782], Loss: 1.5001\n",
            "Epoch [99/300], Step [700/782], Loss: 1.4927\n",
            "Epoch [100/300], Step [100/782], Loss: 1.5078\n",
            "Epoch [100/300], Step [200/782], Loss: 1.4771\n",
            "Epoch [100/300], Step [300/782], Loss: 1.4776\n",
            "Epoch [100/300], Step [400/782], Loss: 1.4734\n",
            "Epoch [100/300], Step [500/782], Loss: 1.4819\n",
            "Epoch [100/300], Step [600/782], Loss: 1.5120\n",
            "Epoch [100/300], Step [700/782], Loss: 1.4681\n",
            "Epoch [101/300], Step [100/782], Loss: 1.4806\n",
            "Epoch [101/300], Step [200/782], Loss: 1.4780\n",
            "Epoch [101/300], Step [300/782], Loss: 1.4616\n",
            "Epoch [101/300], Step [400/782], Loss: 1.4681\n",
            "Epoch [101/300], Step [500/782], Loss: 1.5240\n",
            "Epoch [101/300], Step [600/782], Loss: 1.4651\n",
            "Epoch [101/300], Step [700/782], Loss: 1.4933\n",
            "Epoch [102/300], Step [100/782], Loss: 1.4927\n",
            "Epoch [102/300], Step [200/782], Loss: 1.5082\n",
            "Epoch [102/300], Step [300/782], Loss: 1.4927\n",
            "Epoch [102/300], Step [400/782], Loss: 1.4771\n",
            "Epoch [102/300], Step [500/782], Loss: 1.4851\n",
            "Epoch [102/300], Step [600/782], Loss: 1.4927\n",
            "Epoch [102/300], Step [700/782], Loss: 1.5435\n",
            "Epoch [103/300], Step [100/782], Loss: 1.4880\n",
            "Epoch [103/300], Step [200/782], Loss: 1.5069\n",
            "Epoch [103/300], Step [300/782], Loss: 1.4819\n",
            "Epoch [103/300], Step [400/782], Loss: 1.5370\n",
            "Epoch [103/300], Step [500/782], Loss: 1.4630\n",
            "Epoch [103/300], Step [600/782], Loss: 1.5096\n",
            "Epoch [103/300], Step [700/782], Loss: 1.4775\n",
            "Epoch [104/300], Step [100/782], Loss: 1.5009\n",
            "Epoch [104/300], Step [200/782], Loss: 1.4633\n",
            "Epoch [104/300], Step [300/782], Loss: 1.4821\n",
            "Epoch [104/300], Step [400/782], Loss: 1.5122\n",
            "Epoch [104/300], Step [500/782], Loss: 1.5078\n",
            "Epoch [104/300], Step [600/782], Loss: 1.5020\n",
            "Epoch [104/300], Step [700/782], Loss: 1.4775\n",
            "Epoch [105/300], Step [100/782], Loss: 1.4941\n",
            "Epoch [105/300], Step [200/782], Loss: 1.4649\n",
            "Epoch [105/300], Step [300/782], Loss: 1.4977\n",
            "Epoch [105/300], Step [400/782], Loss: 1.4788\n",
            "Epoch [105/300], Step [500/782], Loss: 1.4804\n",
            "Epoch [105/300], Step [600/782], Loss: 1.4933\n",
            "Epoch [105/300], Step [700/782], Loss: 1.4621\n",
            "Epoch [106/300], Step [100/782], Loss: 1.4620\n",
            "Epoch [106/300], Step [200/782], Loss: 1.4617\n",
            "Epoch [106/300], Step [300/782], Loss: 1.4818\n",
            "Epoch [106/300], Step [400/782], Loss: 1.4947\n",
            "Epoch [106/300], Step [500/782], Loss: 1.4832\n",
            "Epoch [106/300], Step [600/782], Loss: 1.4956\n",
            "Epoch [106/300], Step [700/782], Loss: 1.4628\n",
            "Epoch [107/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [107/300], Step [200/782], Loss: 1.4927\n",
            "Epoch [107/300], Step [300/782], Loss: 1.4922\n",
            "Epoch [107/300], Step [400/782], Loss: 1.4632\n",
            "Epoch [107/300], Step [500/782], Loss: 1.5103\n",
            "Epoch [107/300], Step [600/782], Loss: 1.4807\n",
            "Epoch [107/300], Step [700/782], Loss: 1.5383\n",
            "Epoch [108/300], Step [100/782], Loss: 1.5192\n",
            "Epoch [108/300], Step [200/782], Loss: 1.4822\n",
            "Epoch [108/300], Step [300/782], Loss: 1.4934\n",
            "Epoch [108/300], Step [400/782], Loss: 1.4779\n",
            "Epoch [108/300], Step [500/782], Loss: 1.5116\n",
            "Epoch [108/300], Step [600/782], Loss: 1.4619\n",
            "Epoch [108/300], Step [700/782], Loss: 1.4675\n",
            "Epoch [109/300], Step [100/782], Loss: 1.4781\n",
            "Epoch [109/300], Step [200/782], Loss: 1.4671\n",
            "Epoch [109/300], Step [300/782], Loss: 1.4850\n",
            "Epoch [109/300], Step [400/782], Loss: 1.4781\n",
            "Epoch [109/300], Step [500/782], Loss: 1.4770\n",
            "Epoch [109/300], Step [600/782], Loss: 1.4780\n",
            "Epoch [109/300], Step [700/782], Loss: 1.4790\n",
            "Epoch [110/300], Step [100/782], Loss: 1.4976\n",
            "Epoch [110/300], Step [200/782], Loss: 1.5006\n",
            "Epoch [110/300], Step [300/782], Loss: 1.4886\n",
            "Epoch [110/300], Step [400/782], Loss: 1.4927\n",
            "Epoch [110/300], Step [500/782], Loss: 1.4892\n",
            "Epoch [110/300], Step [600/782], Loss: 1.5086\n",
            "Epoch [110/300], Step [700/782], Loss: 1.4745\n",
            "Epoch [111/300], Step [100/782], Loss: 1.5060\n",
            "Epoch [111/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [111/300], Step [300/782], Loss: 1.4814\n",
            "Epoch [111/300], Step [400/782], Loss: 1.4775\n",
            "Epoch [111/300], Step [500/782], Loss: 1.4772\n",
            "Epoch [111/300], Step [600/782], Loss: 1.5058\n",
            "Epoch [111/300], Step [700/782], Loss: 1.4846\n",
            "Epoch [112/300], Step [100/782], Loss: 1.4930\n",
            "Epoch [112/300], Step [200/782], Loss: 1.4659\n",
            "Epoch [112/300], Step [300/782], Loss: 1.4937\n",
            "Epoch [112/300], Step [400/782], Loss: 1.4792\n",
            "Epoch [112/300], Step [500/782], Loss: 1.4924\n",
            "Epoch [112/300], Step [600/782], Loss: 1.4762\n",
            "Epoch [112/300], Step [700/782], Loss: 1.4773\n",
            "Epoch [113/300], Step [100/782], Loss: 1.4626\n",
            "Epoch [113/300], Step [200/782], Loss: 1.5251\n",
            "Epoch [113/300], Step [300/782], Loss: 1.4941\n",
            "Epoch [113/300], Step [400/782], Loss: 1.4778\n",
            "Epoch [113/300], Step [500/782], Loss: 1.4834\n",
            "Epoch [113/300], Step [600/782], Loss: 1.4940\n",
            "Epoch [113/300], Step [700/782], Loss: 1.4832\n",
            "Epoch [114/300], Step [100/782], Loss: 1.5114\n",
            "Epoch [114/300], Step [200/782], Loss: 1.4807\n",
            "Epoch [114/300], Step [300/782], Loss: 1.4772\n",
            "Epoch [114/300], Step [400/782], Loss: 1.4919\n",
            "Epoch [114/300], Step [500/782], Loss: 1.4720\n",
            "Epoch [114/300], Step [600/782], Loss: 1.4928\n",
            "Epoch [114/300], Step [700/782], Loss: 1.4805\n",
            "Epoch [115/300], Step [100/782], Loss: 1.4623\n",
            "Epoch [115/300], Step [200/782], Loss: 1.4622\n",
            "Epoch [115/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [115/300], Step [400/782], Loss: 1.4774\n",
            "Epoch [115/300], Step [500/782], Loss: 1.4842\n",
            "Epoch [115/300], Step [600/782], Loss: 1.4927\n",
            "Epoch [115/300], Step [700/782], Loss: 1.4709\n",
            "Epoch [116/300], Step [100/782], Loss: 1.5243\n",
            "Epoch [116/300], Step [200/782], Loss: 1.5054\n",
            "Epoch [116/300], Step [300/782], Loss: 1.4812\n",
            "Epoch [116/300], Step [400/782], Loss: 1.4718\n",
            "Epoch [116/300], Step [500/782], Loss: 1.4823\n",
            "Epoch [116/300], Step [600/782], Loss: 1.4923\n",
            "Epoch [116/300], Step [700/782], Loss: 1.4781\n",
            "Epoch [117/300], Step [100/782], Loss: 1.4795\n",
            "Epoch [117/300], Step [200/782], Loss: 1.4621\n",
            "Epoch [117/300], Step [300/782], Loss: 1.5062\n",
            "Epoch [117/300], Step [400/782], Loss: 1.5148\n",
            "Epoch [117/300], Step [500/782], Loss: 1.4692\n",
            "Epoch [117/300], Step [600/782], Loss: 1.4932\n",
            "Epoch [117/300], Step [700/782], Loss: 1.4777\n",
            "Epoch [118/300], Step [100/782], Loss: 1.4765\n",
            "Epoch [118/300], Step [200/782], Loss: 1.4799\n",
            "Epoch [118/300], Step [300/782], Loss: 1.4979\n",
            "Epoch [118/300], Step [400/782], Loss: 1.4776\n",
            "Epoch [118/300], Step [500/782], Loss: 1.4631\n",
            "Epoch [118/300], Step [600/782], Loss: 1.4616\n",
            "Epoch [118/300], Step [700/782], Loss: 1.4782\n",
            "Epoch [119/300], Step [100/782], Loss: 1.4916\n",
            "Epoch [119/300], Step [200/782], Loss: 1.4813\n",
            "Epoch [119/300], Step [300/782], Loss: 1.4684\n",
            "Epoch [119/300], Step [400/782], Loss: 1.4916\n",
            "Epoch [119/300], Step [500/782], Loss: 1.4954\n",
            "Epoch [119/300], Step [600/782], Loss: 1.4776\n",
            "Epoch [119/300], Step [700/782], Loss: 1.4754\n",
            "Epoch [120/300], Step [100/782], Loss: 1.4788\n",
            "Epoch [120/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [120/300], Step [300/782], Loss: 1.4929\n",
            "Epoch [120/300], Step [400/782], Loss: 1.4635\n",
            "Epoch [120/300], Step [500/782], Loss: 1.4781\n",
            "Epoch [120/300], Step [600/782], Loss: 1.4788\n",
            "Epoch [120/300], Step [700/782], Loss: 1.4783\n",
            "Epoch [121/300], Step [100/782], Loss: 1.4650\n",
            "Epoch [121/300], Step [200/782], Loss: 1.4765\n",
            "Epoch [121/300], Step [300/782], Loss: 1.5055\n",
            "Epoch [121/300], Step [400/782], Loss: 1.4787\n",
            "Epoch [121/300], Step [500/782], Loss: 1.4965\n",
            "Epoch [121/300], Step [600/782], Loss: 1.5078\n",
            "Epoch [121/300], Step [700/782], Loss: 1.4633\n",
            "Epoch [122/300], Step [100/782], Loss: 1.4924\n",
            "Epoch [122/300], Step [200/782], Loss: 1.4940\n",
            "Epoch [122/300], Step [300/782], Loss: 1.4792\n",
            "Epoch [122/300], Step [400/782], Loss: 1.4980\n",
            "Epoch [122/300], Step [500/782], Loss: 1.4627\n",
            "Epoch [122/300], Step [600/782], Loss: 1.4647\n",
            "Epoch [122/300], Step [700/782], Loss: 1.4942\n",
            "Epoch [123/300], Step [100/782], Loss: 1.4787\n",
            "Epoch [123/300], Step [200/782], Loss: 1.4784\n",
            "Epoch [123/300], Step [300/782], Loss: 1.4621\n",
            "Epoch [123/300], Step [400/782], Loss: 1.4684\n",
            "Epoch [123/300], Step [500/782], Loss: 1.5093\n",
            "Epoch [123/300], Step [600/782], Loss: 1.4825\n",
            "Epoch [123/300], Step [700/782], Loss: 1.4919\n",
            "Epoch [124/300], Step [100/782], Loss: 1.4776\n",
            "Epoch [124/300], Step [200/782], Loss: 1.4769\n",
            "Epoch [124/300], Step [300/782], Loss: 1.4781\n",
            "Epoch [124/300], Step [400/782], Loss: 1.4940\n",
            "Epoch [124/300], Step [500/782], Loss: 1.4780\n",
            "Epoch [124/300], Step [600/782], Loss: 1.4924\n",
            "Epoch [124/300], Step [700/782], Loss: 1.4708\n",
            "Epoch [125/300], Step [100/782], Loss: 1.4801\n",
            "Epoch [125/300], Step [200/782], Loss: 1.4778\n",
            "Epoch [125/300], Step [300/782], Loss: 1.4617\n",
            "Epoch [125/300], Step [400/782], Loss: 1.4944\n",
            "Epoch [125/300], Step [500/782], Loss: 1.4778\n",
            "Epoch [125/300], Step [600/782], Loss: 1.4786\n",
            "Epoch [125/300], Step [700/782], Loss: 1.4753\n",
            "Epoch [126/300], Step [100/782], Loss: 1.4786\n",
            "Epoch [126/300], Step [200/782], Loss: 1.4928\n",
            "Epoch [126/300], Step [300/782], Loss: 1.4614\n",
            "Epoch [126/300], Step [400/782], Loss: 1.4772\n",
            "Epoch [126/300], Step [500/782], Loss: 1.4682\n",
            "Epoch [126/300], Step [600/782], Loss: 1.4802\n",
            "Epoch [126/300], Step [700/782], Loss: 1.4769\n",
            "Epoch [127/300], Step [100/782], Loss: 1.4770\n",
            "Epoch [127/300], Step [200/782], Loss: 1.4645\n",
            "Epoch [127/300], Step [300/782], Loss: 1.4624\n",
            "Epoch [127/300], Step [400/782], Loss: 1.4841\n",
            "Epoch [127/300], Step [500/782], Loss: 1.4620\n",
            "Epoch [127/300], Step [600/782], Loss: 1.4727\n",
            "Epoch [127/300], Step [700/782], Loss: 1.4943\n",
            "Epoch [128/300], Step [100/782], Loss: 1.4784\n",
            "Epoch [128/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [128/300], Step [300/782], Loss: 1.4764\n",
            "Epoch [128/300], Step [400/782], Loss: 1.4936\n",
            "Epoch [128/300], Step [500/782], Loss: 1.4617\n",
            "Epoch [128/300], Step [600/782], Loss: 1.4787\n",
            "Epoch [128/300], Step [700/782], Loss: 1.4919\n",
            "Epoch [129/300], Step [100/782], Loss: 1.4618\n",
            "Epoch [129/300], Step [200/782], Loss: 1.4617\n",
            "Epoch [129/300], Step [300/782], Loss: 1.4921\n",
            "Epoch [129/300], Step [400/782], Loss: 1.4950\n",
            "Epoch [129/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [129/300], Step [600/782], Loss: 1.5309\n",
            "Epoch [129/300], Step [700/782], Loss: 1.4768\n",
            "Epoch [130/300], Step [100/782], Loss: 1.4950\n",
            "Epoch [130/300], Step [200/782], Loss: 1.5034\n",
            "Epoch [130/300], Step [300/782], Loss: 1.4947\n",
            "Epoch [130/300], Step [400/782], Loss: 1.4985\n",
            "Epoch [130/300], Step [500/782], Loss: 1.4765\n",
            "Epoch [130/300], Step [600/782], Loss: 1.4971\n",
            "Epoch [130/300], Step [700/782], Loss: 1.4792\n",
            "Epoch [131/300], Step [100/782], Loss: 1.4797\n",
            "Epoch [131/300], Step [200/782], Loss: 1.4920\n",
            "Epoch [131/300], Step [300/782], Loss: 1.4819\n",
            "Epoch [131/300], Step [400/782], Loss: 1.4803\n",
            "Epoch [131/300], Step [500/782], Loss: 1.4619\n",
            "Epoch [131/300], Step [600/782], Loss: 1.4652\n",
            "Epoch [131/300], Step [700/782], Loss: 1.4779\n",
            "Epoch [132/300], Step [100/782], Loss: 1.4682\n",
            "Epoch [132/300], Step [200/782], Loss: 1.4732\n",
            "Epoch [132/300], Step [300/782], Loss: 1.4634\n",
            "Epoch [132/300], Step [400/782], Loss: 1.4667\n",
            "Epoch [132/300], Step [500/782], Loss: 1.4624\n",
            "Epoch [132/300], Step [600/782], Loss: 1.4625\n",
            "Epoch [132/300], Step [700/782], Loss: 1.4929\n",
            "Epoch [133/300], Step [100/782], Loss: 1.5170\n",
            "Epoch [133/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [133/300], Step [300/782], Loss: 1.4921\n",
            "Epoch [133/300], Step [400/782], Loss: 1.4771\n",
            "Epoch [133/300], Step [500/782], Loss: 1.4775\n",
            "Epoch [133/300], Step [600/782], Loss: 1.4925\n",
            "Epoch [133/300], Step [700/782], Loss: 1.4621\n",
            "Epoch [134/300], Step [100/782], Loss: 1.4923\n",
            "Epoch [134/300], Step [200/782], Loss: 1.4639\n",
            "Epoch [134/300], Step [300/782], Loss: 1.5007\n",
            "Epoch [134/300], Step [400/782], Loss: 1.4960\n",
            "Epoch [134/300], Step [500/782], Loss: 1.5001\n",
            "Epoch [134/300], Step [600/782], Loss: 1.5083\n",
            "Epoch [134/300], Step [700/782], Loss: 1.4926\n",
            "Epoch [135/300], Step [100/782], Loss: 1.4783\n",
            "Epoch [135/300], Step [200/782], Loss: 1.5281\n",
            "Epoch [135/300], Step [300/782], Loss: 1.5390\n",
            "Epoch [135/300], Step [400/782], Loss: 1.4836\n",
            "Epoch [135/300], Step [500/782], Loss: 1.5000\n",
            "Epoch [135/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [135/300], Step [700/782], Loss: 1.4936\n",
            "Epoch [136/300], Step [100/782], Loss: 1.4633\n",
            "Epoch [136/300], Step [200/782], Loss: 1.4947\n",
            "Epoch [136/300], Step [300/782], Loss: 1.4658\n",
            "Epoch [136/300], Step [400/782], Loss: 1.5101\n",
            "Epoch [136/300], Step [500/782], Loss: 1.4770\n",
            "Epoch [136/300], Step [600/782], Loss: 1.4618\n",
            "Epoch [136/300], Step [700/782], Loss: 1.4774\n",
            "Epoch [137/300], Step [100/782], Loss: 1.4768\n",
            "Epoch [137/300], Step [200/782], Loss: 1.5106\n",
            "Epoch [137/300], Step [300/782], Loss: 1.4614\n",
            "Epoch [137/300], Step [400/782], Loss: 1.4646\n",
            "Epoch [137/300], Step [500/782], Loss: 1.4757\n",
            "Epoch [137/300], Step [600/782], Loss: 1.4770\n",
            "Epoch [137/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [138/300], Step [100/782], Loss: 1.5123\n",
            "Epoch [138/300], Step [200/782], Loss: 1.4789\n",
            "Epoch [138/300], Step [300/782], Loss: 1.4670\n",
            "Epoch [138/300], Step [400/782], Loss: 1.4837\n",
            "Epoch [138/300], Step [500/782], Loss: 1.4633\n",
            "Epoch [138/300], Step [600/782], Loss: 1.4797\n",
            "Epoch [138/300], Step [700/782], Loss: 1.4633\n",
            "Epoch [139/300], Step [100/782], Loss: 1.4766\n",
            "Epoch [139/300], Step [200/782], Loss: 1.4931\n",
            "Epoch [139/300], Step [300/782], Loss: 1.4916\n",
            "Epoch [139/300], Step [400/782], Loss: 1.4716\n",
            "Epoch [139/300], Step [500/782], Loss: 1.4776\n",
            "Epoch [139/300], Step [600/782], Loss: 1.4926\n",
            "Epoch [139/300], Step [700/782], Loss: 1.5074\n",
            "Epoch [140/300], Step [100/782], Loss: 1.4785\n",
            "Epoch [140/300], Step [200/782], Loss: 1.4761\n",
            "Epoch [140/300], Step [300/782], Loss: 1.4620\n",
            "Epoch [140/300], Step [400/782], Loss: 1.5080\n",
            "Epoch [140/300], Step [500/782], Loss: 1.4902\n",
            "Epoch [140/300], Step [600/782], Loss: 1.4787\n",
            "Epoch [140/300], Step [700/782], Loss: 1.4617\n",
            "Epoch [141/300], Step [100/782], Loss: 1.4625\n",
            "Epoch [141/300], Step [200/782], Loss: 1.4636\n",
            "Epoch [141/300], Step [300/782], Loss: 1.4647\n",
            "Epoch [141/300], Step [400/782], Loss: 1.4900\n",
            "Epoch [141/300], Step [500/782], Loss: 1.4771\n",
            "Epoch [141/300], Step [600/782], Loss: 1.4623\n",
            "Epoch [141/300], Step [700/782], Loss: 1.4851\n",
            "Epoch [142/300], Step [100/782], Loss: 1.4925\n",
            "Epoch [142/300], Step [200/782], Loss: 1.4791\n",
            "Epoch [142/300], Step [300/782], Loss: 1.4925\n",
            "Epoch [142/300], Step [400/782], Loss: 1.4779\n",
            "Epoch [142/300], Step [500/782], Loss: 1.4614\n",
            "Epoch [142/300], Step [600/782], Loss: 1.4937\n",
            "Epoch [142/300], Step [700/782], Loss: 1.4630\n",
            "Epoch [143/300], Step [100/782], Loss: 1.4691\n",
            "Epoch [143/300], Step [200/782], Loss: 1.4923\n",
            "Epoch [143/300], Step [300/782], Loss: 1.5160\n",
            "Epoch [143/300], Step [400/782], Loss: 1.4646\n",
            "Epoch [143/300], Step [500/782], Loss: 1.4850\n",
            "Epoch [143/300], Step [600/782], Loss: 1.4767\n",
            "Epoch [143/300], Step [700/782], Loss: 1.4754\n",
            "Epoch [144/300], Step [100/782], Loss: 1.4672\n",
            "Epoch [144/300], Step [200/782], Loss: 1.4655\n",
            "Epoch [144/300], Step [300/782], Loss: 1.4775\n",
            "Epoch [144/300], Step [400/782], Loss: 1.5034\n",
            "Epoch [144/300], Step [500/782], Loss: 1.4618\n",
            "Epoch [144/300], Step [600/782], Loss: 1.4773\n",
            "Epoch [144/300], Step [700/782], Loss: 1.4616\n",
            "Epoch [145/300], Step [100/782], Loss: 1.4628\n",
            "Epoch [145/300], Step [200/782], Loss: 1.4808\n",
            "Epoch [145/300], Step [300/782], Loss: 1.4925\n",
            "Epoch [145/300], Step [400/782], Loss: 1.4773\n",
            "Epoch [145/300], Step [500/782], Loss: 1.4619\n",
            "Epoch [145/300], Step [600/782], Loss: 1.5236\n",
            "Epoch [145/300], Step [700/782], Loss: 1.4770\n",
            "Epoch [146/300], Step [100/782], Loss: 1.4775\n",
            "Epoch [146/300], Step [200/782], Loss: 1.4926\n",
            "Epoch [146/300], Step [300/782], Loss: 1.4640\n",
            "Epoch [146/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [146/300], Step [500/782], Loss: 1.4762\n",
            "Epoch [146/300], Step [600/782], Loss: 1.4805\n",
            "Epoch [146/300], Step [700/782], Loss: 1.5316\n",
            "Epoch [147/300], Step [100/782], Loss: 1.4942\n",
            "Epoch [147/300], Step [200/782], Loss: 1.4762\n",
            "Epoch [147/300], Step [300/782], Loss: 1.4625\n",
            "Epoch [147/300], Step [400/782], Loss: 1.4921\n",
            "Epoch [147/300], Step [500/782], Loss: 1.4693\n",
            "Epoch [147/300], Step [600/782], Loss: 1.4756\n",
            "Epoch [147/300], Step [700/782], Loss: 1.4620\n",
            "Epoch [148/300], Step [100/782], Loss: 1.4764\n",
            "Epoch [148/300], Step [200/782], Loss: 1.4661\n",
            "Epoch [148/300], Step [300/782], Loss: 1.4774\n",
            "Epoch [148/300], Step [400/782], Loss: 1.4615\n",
            "Epoch [148/300], Step [500/782], Loss: 1.4713\n",
            "Epoch [148/300], Step [600/782], Loss: 1.4629\n",
            "Epoch [148/300], Step [700/782], Loss: 1.4613\n",
            "Epoch [149/300], Step [100/782], Loss: 1.5072\n",
            "Epoch [149/300], Step [200/782], Loss: 1.4762\n",
            "Epoch [149/300], Step [300/782], Loss: 1.4623\n",
            "Epoch [149/300], Step [400/782], Loss: 1.4616\n",
            "Epoch [149/300], Step [500/782], Loss: 1.5090\n",
            "Epoch [149/300], Step [600/782], Loss: 1.4773\n",
            "Epoch [149/300], Step [700/782], Loss: 1.4769\n",
            "Epoch [150/300], Step [100/782], Loss: 1.4835\n",
            "Epoch [150/300], Step [200/782], Loss: 1.4786\n",
            "Epoch [150/300], Step [300/782], Loss: 1.5040\n",
            "Epoch [150/300], Step [400/782], Loss: 1.4621\n",
            "Epoch [150/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [150/300], Step [600/782], Loss: 1.4616\n",
            "Epoch [150/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [151/300], Step [100/782], Loss: 1.4932\n",
            "Epoch [151/300], Step [200/782], Loss: 1.4615\n",
            "Epoch [151/300], Step [300/782], Loss: 1.4797\n",
            "Epoch [151/300], Step [400/782], Loss: 1.4763\n",
            "Epoch [151/300], Step [500/782], Loss: 1.4773\n",
            "Epoch [151/300], Step [600/782], Loss: 1.4625\n",
            "Epoch [151/300], Step [700/782], Loss: 1.4775\n",
            "Epoch [152/300], Step [100/782], Loss: 1.4647\n",
            "Epoch [152/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [152/300], Step [300/782], Loss: 1.4778\n",
            "Epoch [152/300], Step [400/782], Loss: 1.4779\n",
            "Epoch [152/300], Step [500/782], Loss: 1.4637\n",
            "Epoch [152/300], Step [600/782], Loss: 1.5106\n",
            "Epoch [152/300], Step [700/782], Loss: 1.5076\n",
            "Epoch [153/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [153/300], Step [200/782], Loss: 1.4617\n",
            "Epoch [153/300], Step [300/782], Loss: 1.4770\n",
            "Epoch [153/300], Step [400/782], Loss: 1.4783\n",
            "Epoch [153/300], Step [500/782], Loss: 1.4777\n",
            "Epoch [153/300], Step [600/782], Loss: 1.4656\n",
            "Epoch [153/300], Step [700/782], Loss: 1.4769\n",
            "Epoch [154/300], Step [100/782], Loss: 1.4625\n",
            "Epoch [154/300], Step [200/782], Loss: 1.4827\n",
            "Epoch [154/300], Step [300/782], Loss: 1.4770\n",
            "Epoch [154/300], Step [400/782], Loss: 1.4641\n",
            "Epoch [154/300], Step [500/782], Loss: 1.4884\n",
            "Epoch [154/300], Step [600/782], Loss: 1.4624\n",
            "Epoch [154/300], Step [700/782], Loss: 1.4946\n",
            "Epoch [155/300], Step [100/782], Loss: 1.4627\n",
            "Epoch [155/300], Step [200/782], Loss: 1.4691\n",
            "Epoch [155/300], Step [300/782], Loss: 1.4782\n",
            "Epoch [155/300], Step [400/782], Loss: 1.4616\n",
            "Epoch [155/300], Step [500/782], Loss: 1.5238\n",
            "Epoch [155/300], Step [600/782], Loss: 1.4619\n",
            "Epoch [155/300], Step [700/782], Loss: 1.4628\n",
            "Epoch [156/300], Step [100/782], Loss: 1.4772\n",
            "Epoch [156/300], Step [200/782], Loss: 1.4831\n",
            "Epoch [156/300], Step [300/782], Loss: 1.5080\n",
            "Epoch [156/300], Step [400/782], Loss: 1.4781\n",
            "Epoch [156/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [156/300], Step [600/782], Loss: 1.4772\n",
            "Epoch [156/300], Step [700/782], Loss: 1.4764\n",
            "Epoch [157/300], Step [100/782], Loss: 1.4636\n",
            "Epoch [157/300], Step [200/782], Loss: 1.5082\n",
            "Epoch [157/300], Step [300/782], Loss: 1.4765\n",
            "Epoch [157/300], Step [400/782], Loss: 1.4762\n",
            "Epoch [157/300], Step [500/782], Loss: 1.4618\n",
            "Epoch [157/300], Step [600/782], Loss: 1.4618\n",
            "Epoch [157/300], Step [700/782], Loss: 1.4783\n",
            "Epoch [158/300], Step [100/782], Loss: 1.4647\n",
            "Epoch [158/300], Step [200/782], Loss: 1.4769\n",
            "Epoch [158/300], Step [300/782], Loss: 1.5062\n",
            "Epoch [158/300], Step [400/782], Loss: 1.4924\n",
            "Epoch [158/300], Step [500/782], Loss: 1.4921\n",
            "Epoch [158/300], Step [600/782], Loss: 1.4929\n",
            "Epoch [158/300], Step [700/782], Loss: 1.4621\n",
            "Epoch [159/300], Step [100/782], Loss: 1.4926\n",
            "Epoch [159/300], Step [200/782], Loss: 1.4768\n",
            "Epoch [159/300], Step [300/782], Loss: 1.4929\n",
            "Epoch [159/300], Step [400/782], Loss: 1.4968\n",
            "Epoch [159/300], Step [500/782], Loss: 1.4759\n",
            "Epoch [159/300], Step [600/782], Loss: 1.4769\n",
            "Epoch [159/300], Step [700/782], Loss: 1.4920\n",
            "Epoch [160/300], Step [100/782], Loss: 1.4920\n",
            "Epoch [160/300], Step [200/782], Loss: 1.4614\n",
            "Epoch [160/300], Step [300/782], Loss: 1.4762\n",
            "Epoch [160/300], Step [400/782], Loss: 1.5071\n",
            "Epoch [160/300], Step [500/782], Loss: 1.4659\n",
            "Epoch [160/300], Step [600/782], Loss: 1.4921\n",
            "Epoch [160/300], Step [700/782], Loss: 1.4918\n",
            "Epoch [161/300], Step [100/782], Loss: 1.4615\n",
            "Epoch [161/300], Step [200/782], Loss: 1.4766\n",
            "Epoch [161/300], Step [300/782], Loss: 1.5010\n",
            "Epoch [161/300], Step [400/782], Loss: 1.4998\n",
            "Epoch [161/300], Step [500/782], Loss: 1.4768\n",
            "Epoch [161/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [161/300], Step [700/782], Loss: 1.4684\n",
            "Epoch [162/300], Step [100/782], Loss: 1.4828\n",
            "Epoch [162/300], Step [200/782], Loss: 1.4934\n",
            "Epoch [162/300], Step [300/782], Loss: 1.4619\n",
            "Epoch [162/300], Step [400/782], Loss: 1.4671\n",
            "Epoch [162/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [162/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [162/300], Step [700/782], Loss: 1.4920\n",
            "Epoch [163/300], Step [100/782], Loss: 1.4772\n",
            "Epoch [163/300], Step [200/782], Loss: 1.4644\n",
            "Epoch [163/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [163/300], Step [400/782], Loss: 1.4619\n",
            "Epoch [163/300], Step [500/782], Loss: 1.4766\n",
            "Epoch [163/300], Step [600/782], Loss: 1.4981\n",
            "Epoch [163/300], Step [700/782], Loss: 1.4785\n",
            "Epoch [164/300], Step [100/782], Loss: 1.4787\n",
            "Epoch [164/300], Step [200/782], Loss: 1.4781\n",
            "Epoch [164/300], Step [300/782], Loss: 1.4935\n",
            "Epoch [164/300], Step [400/782], Loss: 1.4782\n",
            "Epoch [164/300], Step [500/782], Loss: 1.4619\n",
            "Epoch [164/300], Step [600/782], Loss: 1.5006\n",
            "Epoch [164/300], Step [700/782], Loss: 1.4769\n",
            "Epoch [165/300], Step [100/782], Loss: 1.4666\n",
            "Epoch [165/300], Step [200/782], Loss: 1.4612\n",
            "Epoch [165/300], Step [300/782], Loss: 1.4825\n",
            "Epoch [165/300], Step [400/782], Loss: 1.4626\n",
            "Epoch [165/300], Step [500/782], Loss: 1.4613\n",
            "Epoch [165/300], Step [600/782], Loss: 1.4777\n",
            "Epoch [165/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [166/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [166/300], Step [200/782], Loss: 1.5077\n",
            "Epoch [166/300], Step [300/782], Loss: 1.4615\n",
            "Epoch [166/300], Step [400/782], Loss: 1.4784\n",
            "Epoch [166/300], Step [500/782], Loss: 1.5087\n",
            "Epoch [166/300], Step [600/782], Loss: 1.4762\n",
            "Epoch [166/300], Step [700/782], Loss: 1.4773\n",
            "Epoch [167/300], Step [100/782], Loss: 1.4924\n",
            "Epoch [167/300], Step [200/782], Loss: 1.4693\n",
            "Epoch [167/300], Step [300/782], Loss: 1.4659\n",
            "Epoch [167/300], Step [400/782], Loss: 1.5090\n",
            "Epoch [167/300], Step [500/782], Loss: 1.5118\n",
            "Epoch [167/300], Step [600/782], Loss: 1.4769\n",
            "Epoch [167/300], Step [700/782], Loss: 1.4831\n",
            "Epoch [168/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [168/300], Step [200/782], Loss: 1.4927\n",
            "Epoch [168/300], Step [300/782], Loss: 1.4765\n",
            "Epoch [168/300], Step [400/782], Loss: 1.4959\n",
            "Epoch [168/300], Step [500/782], Loss: 1.4769\n",
            "Epoch [168/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [168/300], Step [700/782], Loss: 1.4653\n",
            "Epoch [169/300], Step [100/782], Loss: 1.4626\n",
            "Epoch [169/300], Step [200/782], Loss: 1.4787\n",
            "Epoch [169/300], Step [300/782], Loss: 1.4967\n",
            "Epoch [169/300], Step [400/782], Loss: 1.4878\n",
            "Epoch [169/300], Step [500/782], Loss: 1.5081\n",
            "Epoch [169/300], Step [600/782], Loss: 1.4799\n",
            "Epoch [169/300], Step [700/782], Loss: 1.4773\n",
            "Epoch [170/300], Step [100/782], Loss: 1.4619\n",
            "Epoch [170/300], Step [200/782], Loss: 1.4616\n",
            "Epoch [170/300], Step [300/782], Loss: 1.4614\n",
            "Epoch [170/300], Step [400/782], Loss: 1.5092\n",
            "Epoch [170/300], Step [500/782], Loss: 1.4821\n",
            "Epoch [170/300], Step [600/782], Loss: 1.4632\n",
            "Epoch [170/300], Step [700/782], Loss: 1.4613\n",
            "Epoch [171/300], Step [100/782], Loss: 1.4745\n",
            "Epoch [171/300], Step [200/782], Loss: 1.4781\n",
            "Epoch [171/300], Step [300/782], Loss: 1.4660\n",
            "Epoch [171/300], Step [400/782], Loss: 1.5077\n",
            "Epoch [171/300], Step [500/782], Loss: 1.4773\n",
            "Epoch [171/300], Step [600/782], Loss: 1.4615\n",
            "Epoch [171/300], Step [700/782], Loss: 1.5106\n",
            "Epoch [172/300], Step [100/782], Loss: 1.4950\n",
            "Epoch [172/300], Step [200/782], Loss: 1.4618\n",
            "Epoch [172/300], Step [300/782], Loss: 1.4766\n",
            "Epoch [172/300], Step [400/782], Loss: 1.4628\n",
            "Epoch [172/300], Step [500/782], Loss: 1.4802\n",
            "Epoch [172/300], Step [600/782], Loss: 1.4623\n",
            "Epoch [172/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [173/300], Step [100/782], Loss: 1.4815\n",
            "Epoch [173/300], Step [200/782], Loss: 1.4768\n",
            "Epoch [173/300], Step [300/782], Loss: 1.4933\n",
            "Epoch [173/300], Step [400/782], Loss: 1.4829\n",
            "Epoch [173/300], Step [500/782], Loss: 1.4918\n",
            "Epoch [173/300], Step [600/782], Loss: 1.4787\n",
            "Epoch [173/300], Step [700/782], Loss: 1.4790\n",
            "Epoch [174/300], Step [100/782], Loss: 1.5041\n",
            "Epoch [174/300], Step [200/782], Loss: 1.4765\n",
            "Epoch [174/300], Step [300/782], Loss: 1.4767\n",
            "Epoch [174/300], Step [400/782], Loss: 1.5234\n",
            "Epoch [174/300], Step [500/782], Loss: 1.4613\n",
            "Epoch [174/300], Step [600/782], Loss: 1.4772\n",
            "Epoch [174/300], Step [700/782], Loss: 1.4632\n",
            "Epoch [175/300], Step [100/782], Loss: 1.5078\n",
            "Epoch [175/300], Step [200/782], Loss: 1.4621\n",
            "Epoch [175/300], Step [300/782], Loss: 1.4628\n",
            "Epoch [175/300], Step [400/782], Loss: 1.4770\n",
            "Epoch [175/300], Step [500/782], Loss: 1.4616\n",
            "Epoch [175/300], Step [600/782], Loss: 1.5119\n",
            "Epoch [175/300], Step [700/782], Loss: 1.4620\n",
            "Epoch [176/300], Step [100/782], Loss: 1.4781\n",
            "Epoch [176/300], Step [200/782], Loss: 1.5077\n",
            "Epoch [176/300], Step [300/782], Loss: 1.4782\n",
            "Epoch [176/300], Step [400/782], Loss: 1.4922\n",
            "Epoch [176/300], Step [500/782], Loss: 1.4774\n",
            "Epoch [176/300], Step [600/782], Loss: 1.4625\n",
            "Epoch [176/300], Step [700/782], Loss: 1.4633\n",
            "Epoch [177/300], Step [100/782], Loss: 1.4613\n",
            "Epoch [177/300], Step [200/782], Loss: 1.4615\n",
            "Epoch [177/300], Step [300/782], Loss: 1.4921\n",
            "Epoch [177/300], Step [400/782], Loss: 1.4626\n",
            "Epoch [177/300], Step [500/782], Loss: 1.4802\n",
            "Epoch [177/300], Step [600/782], Loss: 1.5238\n",
            "Epoch [177/300], Step [700/782], Loss: 1.4620\n",
            "Epoch [178/300], Step [100/782], Loss: 1.4738\n",
            "Epoch [178/300], Step [200/782], Loss: 1.4766\n",
            "Epoch [178/300], Step [300/782], Loss: 1.4936\n",
            "Epoch [178/300], Step [400/782], Loss: 1.4925\n",
            "Epoch [178/300], Step [500/782], Loss: 1.4927\n",
            "Epoch [178/300], Step [600/782], Loss: 1.4791\n",
            "Epoch [178/300], Step [700/782], Loss: 1.5170\n",
            "Epoch [179/300], Step [100/782], Loss: 1.4613\n",
            "Epoch [179/300], Step [200/782], Loss: 1.4612\n",
            "Epoch [179/300], Step [300/782], Loss: 1.4613\n",
            "Epoch [179/300], Step [400/782], Loss: 1.4768\n",
            "Epoch [179/300], Step [500/782], Loss: 1.4783\n",
            "Epoch [179/300], Step [600/782], Loss: 1.5003\n",
            "Epoch [179/300], Step [700/782], Loss: 1.4713\n",
            "Epoch [180/300], Step [100/782], Loss: 1.4919\n",
            "Epoch [180/300], Step [200/782], Loss: 1.4775\n",
            "Epoch [180/300], Step [300/782], Loss: 1.4776\n",
            "Epoch [180/300], Step [400/782], Loss: 1.4613\n",
            "Epoch [180/300], Step [500/782], Loss: 1.4773\n",
            "Epoch [180/300], Step [600/782], Loss: 1.4996\n",
            "Epoch [180/300], Step [700/782], Loss: 1.4618\n",
            "Epoch [181/300], Step [100/782], Loss: 1.4613\n",
            "Epoch [181/300], Step [200/782], Loss: 1.4766\n",
            "Epoch [181/300], Step [300/782], Loss: 1.5075\n",
            "Epoch [181/300], Step [400/782], Loss: 1.4769\n",
            "Epoch [181/300], Step [500/782], Loss: 1.4769\n",
            "Epoch [181/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [181/300], Step [700/782], Loss: 1.4770\n",
            "Epoch [182/300], Step [100/782], Loss: 1.4768\n",
            "Epoch [182/300], Step [200/782], Loss: 1.4768\n",
            "Epoch [182/300], Step [300/782], Loss: 1.4764\n",
            "Epoch [182/300], Step [400/782], Loss: 1.4926\n",
            "Epoch [182/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [182/300], Step [600/782], Loss: 1.4615\n",
            "Epoch [182/300], Step [700/782], Loss: 1.4768\n",
            "Epoch [183/300], Step [100/782], Loss: 1.4770\n",
            "Epoch [183/300], Step [200/782], Loss: 1.4788\n",
            "Epoch [183/300], Step [300/782], Loss: 1.4924\n",
            "Epoch [183/300], Step [400/782], Loss: 1.4929\n",
            "Epoch [183/300], Step [500/782], Loss: 1.4804\n",
            "Epoch [183/300], Step [600/782], Loss: 1.4820\n",
            "Epoch [183/300], Step [700/782], Loss: 1.4921\n",
            "Epoch [184/300], Step [100/782], Loss: 1.4644\n",
            "Epoch [184/300], Step [200/782], Loss: 1.4768\n",
            "Epoch [184/300], Step [300/782], Loss: 1.4616\n",
            "Epoch [184/300], Step [400/782], Loss: 1.4929\n",
            "Epoch [184/300], Step [500/782], Loss: 1.4614\n",
            "Epoch [184/300], Step [600/782], Loss: 1.4770\n",
            "Epoch [184/300], Step [700/782], Loss: 1.4613\n",
            "Epoch [185/300], Step [100/782], Loss: 1.4860\n",
            "Epoch [185/300], Step [200/782], Loss: 1.4752\n",
            "Epoch [185/300], Step [300/782], Loss: 1.4614\n",
            "Epoch [185/300], Step [400/782], Loss: 1.4617\n",
            "Epoch [185/300], Step [500/782], Loss: 1.4768\n",
            "Epoch [185/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [185/300], Step [700/782], Loss: 1.4802\n",
            "Epoch [186/300], Step [100/782], Loss: 1.4800\n",
            "Epoch [186/300], Step [200/782], Loss: 1.4768\n",
            "Epoch [186/300], Step [300/782], Loss: 1.4801\n",
            "Epoch [186/300], Step [400/782], Loss: 1.4815\n",
            "Epoch [186/300], Step [500/782], Loss: 1.4931\n",
            "Epoch [186/300], Step [600/782], Loss: 1.4615\n",
            "Epoch [186/300], Step [700/782], Loss: 1.4639\n",
            "Epoch [187/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [187/300], Step [200/782], Loss: 1.4643\n",
            "Epoch [187/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [187/300], Step [400/782], Loss: 1.4632\n",
            "Epoch [187/300], Step [500/782], Loss: 1.4615\n",
            "Epoch [187/300], Step [600/782], Loss: 1.4771\n",
            "Epoch [187/300], Step [700/782], Loss: 1.4908\n",
            "Epoch [188/300], Step [100/782], Loss: 1.4919\n",
            "Epoch [188/300], Step [200/782], Loss: 1.5082\n",
            "Epoch [188/300], Step [300/782], Loss: 1.4924\n",
            "Epoch [188/300], Step [400/782], Loss: 1.4780\n",
            "Epoch [188/300], Step [500/782], Loss: 1.4769\n",
            "Epoch [188/300], Step [600/782], Loss: 1.4661\n",
            "Epoch [188/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [189/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [189/300], Step [200/782], Loss: 1.5316\n",
            "Epoch [189/300], Step [300/782], Loss: 1.4786\n",
            "Epoch [189/300], Step [400/782], Loss: 1.4615\n",
            "Epoch [189/300], Step [500/782], Loss: 1.4766\n",
            "Epoch [189/300], Step [600/782], Loss: 1.4767\n",
            "Epoch [189/300], Step [700/782], Loss: 1.4935\n",
            "Epoch [190/300], Step [100/782], Loss: 1.4924\n",
            "Epoch [190/300], Step [200/782], Loss: 1.4933\n",
            "Epoch [190/300], Step [300/782], Loss: 1.4750\n",
            "Epoch [190/300], Step [400/782], Loss: 1.4918\n",
            "Epoch [190/300], Step [500/782], Loss: 1.4662\n",
            "Epoch [190/300], Step [600/782], Loss: 1.5078\n",
            "Epoch [190/300], Step [700/782], Loss: 1.4810\n",
            "Epoch [191/300], Step [100/782], Loss: 1.4623\n",
            "Epoch [191/300], Step [200/782], Loss: 1.4614\n",
            "Epoch [191/300], Step [300/782], Loss: 1.4727\n",
            "Epoch [191/300], Step [400/782], Loss: 1.4658\n",
            "Epoch [191/300], Step [500/782], Loss: 1.4763\n",
            "Epoch [191/300], Step [600/782], Loss: 1.4677\n",
            "Epoch [191/300], Step [700/782], Loss: 1.4923\n",
            "Epoch [192/300], Step [100/782], Loss: 1.5086\n",
            "Epoch [192/300], Step [200/782], Loss: 1.4768\n",
            "Epoch [192/300], Step [300/782], Loss: 1.4775\n",
            "Epoch [192/300], Step [400/782], Loss: 1.4787\n",
            "Epoch [192/300], Step [500/782], Loss: 1.4949\n",
            "Epoch [192/300], Step [600/782], Loss: 1.4849\n",
            "Epoch [192/300], Step [700/782], Loss: 1.5081\n",
            "Epoch [193/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [193/300], Step [200/782], Loss: 1.4762\n",
            "Epoch [193/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [193/300], Step [400/782], Loss: 1.4613\n",
            "Epoch [193/300], Step [500/782], Loss: 1.4613\n",
            "Epoch [193/300], Step [600/782], Loss: 1.4767\n",
            "Epoch [193/300], Step [700/782], Loss: 1.4776\n",
            "Epoch [194/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [194/300], Step [200/782], Loss: 1.4743\n",
            "Epoch [194/300], Step [300/782], Loss: 1.4925\n",
            "Epoch [194/300], Step [400/782], Loss: 1.4742\n",
            "Epoch [194/300], Step [500/782], Loss: 1.4893\n",
            "Epoch [194/300], Step [600/782], Loss: 1.4625\n",
            "Epoch [194/300], Step [700/782], Loss: 1.4613\n",
            "Epoch [195/300], Step [100/782], Loss: 1.4764\n",
            "Epoch [195/300], Step [200/782], Loss: 1.5118\n",
            "Epoch [195/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [195/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [195/300], Step [500/782], Loss: 1.4766\n",
            "Epoch [195/300], Step [600/782], Loss: 1.4618\n",
            "Epoch [195/300], Step [700/782], Loss: 1.4943\n",
            "Epoch [196/300], Step [100/782], Loss: 1.5105\n",
            "Epoch [196/300], Step [200/782], Loss: 1.5144\n",
            "Epoch [196/300], Step [300/782], Loss: 1.4613\n",
            "Epoch [196/300], Step [400/782], Loss: 1.4768\n",
            "Epoch [196/300], Step [500/782], Loss: 1.4623\n",
            "Epoch [196/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [196/300], Step [700/782], Loss: 1.4616\n",
            "Epoch [197/300], Step [100/782], Loss: 1.4769\n",
            "Epoch [197/300], Step [200/782], Loss: 1.4772\n",
            "Epoch [197/300], Step [300/782], Loss: 1.5194\n",
            "Epoch [197/300], Step [400/782], Loss: 1.4615\n",
            "Epoch [197/300], Step [500/782], Loss: 1.4774\n",
            "Epoch [197/300], Step [600/782], Loss: 1.4621\n",
            "Epoch [197/300], Step [700/782], Loss: 1.4764\n",
            "Epoch [198/300], Step [100/782], Loss: 1.4925\n",
            "Epoch [198/300], Step [200/782], Loss: 1.4614\n",
            "Epoch [198/300], Step [300/782], Loss: 1.4768\n",
            "Epoch [198/300], Step [400/782], Loss: 1.4768\n",
            "Epoch [198/300], Step [500/782], Loss: 1.4768\n",
            "Epoch [198/300], Step [600/782], Loss: 1.4924\n",
            "Epoch [198/300], Step [700/782], Loss: 1.4658\n",
            "Epoch [199/300], Step [100/782], Loss: 1.4748\n",
            "Epoch [199/300], Step [200/782], Loss: 1.4615\n",
            "Epoch [199/300], Step [300/782], Loss: 1.4632\n",
            "Epoch [199/300], Step [400/782], Loss: 1.4768\n",
            "Epoch [199/300], Step [500/782], Loss: 1.4769\n",
            "Epoch [199/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [199/300], Step [700/782], Loss: 1.5208\n",
            "Epoch [200/300], Step [100/782], Loss: 1.4616\n",
            "Epoch [200/300], Step [200/782], Loss: 1.4925\n",
            "Epoch [200/300], Step [300/782], Loss: 1.4778\n",
            "Epoch [200/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [200/300], Step [500/782], Loss: 1.4616\n",
            "Epoch [200/300], Step [600/782], Loss: 1.4687\n",
            "Epoch [200/300], Step [700/782], Loss: 1.4615\n",
            "Epoch [201/300], Step [100/782], Loss: 1.4770\n",
            "Epoch [201/300], Step [200/782], Loss: 1.4765\n",
            "Epoch [201/300], Step [300/782], Loss: 1.4615\n",
            "Epoch [201/300], Step [400/782], Loss: 1.4771\n",
            "Epoch [201/300], Step [500/782], Loss: 1.4619\n",
            "Epoch [201/300], Step [600/782], Loss: 1.4930\n",
            "Epoch [201/300], Step [700/782], Loss: 1.4919\n",
            "Epoch [202/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [202/300], Step [200/782], Loss: 1.5007\n",
            "Epoch [202/300], Step [300/782], Loss: 1.4619\n",
            "Epoch [202/300], Step [400/782], Loss: 1.4774\n",
            "Epoch [202/300], Step [500/782], Loss: 1.5106\n",
            "Epoch [202/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [202/300], Step [700/782], Loss: 1.4619\n",
            "Epoch [203/300], Step [100/782], Loss: 1.4809\n",
            "Epoch [203/300], Step [200/782], Loss: 1.4766\n",
            "Epoch [203/300], Step [300/782], Loss: 1.4613\n",
            "Epoch [203/300], Step [400/782], Loss: 1.4613\n",
            "Epoch [203/300], Step [500/782], Loss: 1.4614\n",
            "Epoch [203/300], Step [600/782], Loss: 1.4619\n",
            "Epoch [203/300], Step [700/782], Loss: 1.4771\n",
            "Epoch [204/300], Step [100/782], Loss: 1.4765\n",
            "Epoch [204/300], Step [200/782], Loss: 1.4939\n",
            "Epoch [204/300], Step [300/782], Loss: 1.4771\n",
            "Epoch [204/300], Step [400/782], Loss: 1.4660\n",
            "Epoch [204/300], Step [500/782], Loss: 1.4839\n",
            "Epoch [204/300], Step [600/782], Loss: 1.4779\n",
            "Epoch [204/300], Step [700/782], Loss: 1.4627\n",
            "Epoch [205/300], Step [100/782], Loss: 1.4767\n",
            "Epoch [205/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [205/300], Step [300/782], Loss: 1.5076\n",
            "Epoch [205/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [205/300], Step [500/782], Loss: 1.5086\n",
            "Epoch [205/300], Step [600/782], Loss: 1.4645\n",
            "Epoch [205/300], Step [700/782], Loss: 1.4619\n",
            "Epoch [206/300], Step [100/782], Loss: 1.4770\n",
            "Epoch [206/300], Step [200/782], Loss: 1.4919\n",
            "Epoch [206/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [206/300], Step [400/782], Loss: 1.4774\n",
            "Epoch [206/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [206/300], Step [600/782], Loss: 1.4766\n",
            "Epoch [206/300], Step [700/782], Loss: 1.4796\n",
            "Epoch [207/300], Step [100/782], Loss: 1.4616\n",
            "Epoch [207/300], Step [200/782], Loss: 1.4767\n",
            "Epoch [207/300], Step [300/782], Loss: 1.4620\n",
            "Epoch [207/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [207/300], Step [500/782], Loss: 1.4614\n",
            "Epoch [207/300], Step [600/782], Loss: 1.4627\n",
            "Epoch [207/300], Step [700/782], Loss: 1.4658\n",
            "Epoch [208/300], Step [100/782], Loss: 1.4768\n",
            "Epoch [208/300], Step [200/782], Loss: 1.4618\n",
            "Epoch [208/300], Step [300/782], Loss: 1.4613\n",
            "Epoch [208/300], Step [400/782], Loss: 1.4615\n",
            "Epoch [208/300], Step [500/782], Loss: 1.5079\n",
            "Epoch [208/300], Step [600/782], Loss: 1.4768\n",
            "Epoch [208/300], Step [700/782], Loss: 1.4617\n",
            "Epoch [209/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [209/300], Step [200/782], Loss: 1.4921\n",
            "Epoch [209/300], Step [300/782], Loss: 1.5232\n",
            "Epoch [209/300], Step [400/782], Loss: 1.4766\n",
            "Epoch [209/300], Step [500/782], Loss: 1.4666\n",
            "Epoch [209/300], Step [600/782], Loss: 1.4927\n",
            "Epoch [209/300], Step [700/782], Loss: 1.4773\n",
            "Epoch [210/300], Step [100/782], Loss: 1.4616\n",
            "Epoch [210/300], Step [200/782], Loss: 1.4767\n",
            "Epoch [210/300], Step [300/782], Loss: 1.4918\n",
            "Epoch [210/300], Step [400/782], Loss: 1.4613\n",
            "Epoch [210/300], Step [500/782], Loss: 1.4637\n",
            "Epoch [210/300], Step [600/782], Loss: 1.4614\n",
            "Epoch [210/300], Step [700/782], Loss: 1.4768\n",
            "Epoch [211/300], Step [100/782], Loss: 1.4924\n",
            "Epoch [211/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [211/300], Step [300/782], Loss: 1.4637\n",
            "Epoch [211/300], Step [400/782], Loss: 1.4769\n",
            "Epoch [211/300], Step [500/782], Loss: 1.4921\n",
            "Epoch [211/300], Step [600/782], Loss: 1.5080\n",
            "Epoch [211/300], Step [700/782], Loss: 1.4638\n",
            "Epoch [212/300], Step [100/782], Loss: 1.4782\n",
            "Epoch [212/300], Step [200/782], Loss: 1.4737\n",
            "Epoch [212/300], Step [300/782], Loss: 1.4626\n",
            "Epoch [212/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [212/300], Step [500/782], Loss: 1.4615\n",
            "Epoch [212/300], Step [600/782], Loss: 1.4615\n",
            "Epoch [212/300], Step [700/782], Loss: 1.4635\n",
            "Epoch [213/300], Step [100/782], Loss: 1.4772\n",
            "Epoch [213/300], Step [200/782], Loss: 1.4786\n",
            "Epoch [213/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [213/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [213/300], Step [500/782], Loss: 1.4862\n",
            "Epoch [213/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [213/300], Step [700/782], Loss: 1.4941\n",
            "Epoch [214/300], Step [100/782], Loss: 1.5086\n",
            "Epoch [214/300], Step [200/782], Loss: 1.4612\n",
            "Epoch [214/300], Step [300/782], Loss: 1.4621\n",
            "Epoch [214/300], Step [400/782], Loss: 1.4767\n",
            "Epoch [214/300], Step [500/782], Loss: 1.4771\n",
            "Epoch [214/300], Step [600/782], Loss: 1.5092\n",
            "Epoch [214/300], Step [700/782], Loss: 1.4975\n",
            "Epoch [215/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [215/300], Step [200/782], Loss: 1.4612\n",
            "Epoch [215/300], Step [300/782], Loss: 1.4669\n",
            "Epoch [215/300], Step [400/782], Loss: 1.4766\n",
            "Epoch [215/300], Step [500/782], Loss: 1.4764\n",
            "Epoch [215/300], Step [600/782], Loss: 1.4616\n",
            "Epoch [215/300], Step [700/782], Loss: 1.5071\n",
            "Epoch [216/300], Step [100/782], Loss: 1.4613\n",
            "Epoch [216/300], Step [200/782], Loss: 1.4769\n",
            "Epoch [216/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [216/300], Step [400/782], Loss: 1.4924\n",
            "Epoch [216/300], Step [500/782], Loss: 1.4765\n",
            "Epoch [216/300], Step [600/782], Loss: 1.5032\n",
            "Epoch [216/300], Step [700/782], Loss: 1.4777\n",
            "Epoch [217/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [217/300], Step [200/782], Loss: 1.5236\n",
            "Epoch [217/300], Step [300/782], Loss: 1.4770\n",
            "Epoch [217/300], Step [400/782], Loss: 1.4772\n",
            "Epoch [217/300], Step [500/782], Loss: 1.4768\n",
            "Epoch [217/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [217/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [218/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [218/300], Step [200/782], Loss: 1.4921\n",
            "Epoch [218/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [218/300], Step [400/782], Loss: 1.4784\n",
            "Epoch [218/300], Step [500/782], Loss: 1.4921\n",
            "Epoch [218/300], Step [600/782], Loss: 1.4766\n",
            "Epoch [218/300], Step [700/782], Loss: 1.4920\n",
            "Epoch [219/300], Step [100/782], Loss: 1.4647\n",
            "Epoch [219/300], Step [200/782], Loss: 1.5067\n",
            "Epoch [219/300], Step [300/782], Loss: 1.5074\n",
            "Epoch [219/300], Step [400/782], Loss: 1.4769\n",
            "Epoch [219/300], Step [500/782], Loss: 1.4633\n",
            "Epoch [219/300], Step [600/782], Loss: 1.4801\n",
            "Epoch [219/300], Step [700/782], Loss: 1.4635\n",
            "Epoch [220/300], Step [100/782], Loss: 1.5554\n",
            "Epoch [220/300], Step [200/782], Loss: 1.4614\n",
            "Epoch [220/300], Step [300/782], Loss: 1.4768\n",
            "Epoch [220/300], Step [400/782], Loss: 1.4614\n",
            "Epoch [220/300], Step [500/782], Loss: 1.4631\n",
            "Epoch [220/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [220/300], Step [700/782], Loss: 1.4924\n",
            "Epoch [221/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [221/300], Step [200/782], Loss: 1.5071\n",
            "Epoch [221/300], Step [300/782], Loss: 1.4626\n",
            "Epoch [221/300], Step [400/782], Loss: 1.4765\n",
            "Epoch [221/300], Step [500/782], Loss: 1.4829\n",
            "Epoch [221/300], Step [600/782], Loss: 1.4918\n",
            "Epoch [221/300], Step [700/782], Loss: 1.4943\n",
            "Epoch [222/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [222/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [222/300], Step [300/782], Loss: 1.4783\n",
            "Epoch [222/300], Step [400/782], Loss: 1.4768\n",
            "Epoch [222/300], Step [500/782], Loss: 1.4771\n",
            "Epoch [222/300], Step [600/782], Loss: 1.4927\n",
            "Epoch [222/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [223/300], Step [100/782], Loss: 1.4797\n",
            "Epoch [223/300], Step [200/782], Loss: 1.4761\n",
            "Epoch [223/300], Step [300/782], Loss: 1.4880\n",
            "Epoch [223/300], Step [400/782], Loss: 1.4687\n",
            "Epoch [223/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [223/300], Step [600/782], Loss: 1.4769\n",
            "Epoch [223/300], Step [700/782], Loss: 1.4923\n",
            "Epoch [224/300], Step [100/782], Loss: 1.4767\n",
            "Epoch [224/300], Step [200/782], Loss: 1.4763\n",
            "Epoch [224/300], Step [300/782], Loss: 1.4615\n",
            "Epoch [224/300], Step [400/782], Loss: 1.4773\n",
            "Epoch [224/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [224/300], Step [600/782], Loss: 1.4616\n",
            "Epoch [224/300], Step [700/782], Loss: 1.4613\n",
            "Epoch [225/300], Step [100/782], Loss: 1.4616\n",
            "Epoch [225/300], Step [200/782], Loss: 1.4655\n",
            "Epoch [225/300], Step [300/782], Loss: 1.4782\n",
            "Epoch [225/300], Step [400/782], Loss: 1.4617\n",
            "Epoch [225/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [225/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [225/300], Step [700/782], Loss: 1.4613\n",
            "Epoch [226/300], Step [100/782], Loss: 1.4631\n",
            "Epoch [226/300], Step [200/782], Loss: 1.5081\n",
            "Epoch [226/300], Step [300/782], Loss: 1.4773\n",
            "Epoch [226/300], Step [400/782], Loss: 1.4775\n",
            "Epoch [226/300], Step [500/782], Loss: 1.4620\n",
            "Epoch [226/300], Step [600/782], Loss: 1.4617\n",
            "Epoch [226/300], Step [700/782], Loss: 1.4768\n",
            "Epoch [227/300], Step [100/782], Loss: 1.4768\n",
            "Epoch [227/300], Step [200/782], Loss: 1.4651\n",
            "Epoch [227/300], Step [300/782], Loss: 1.4916\n",
            "Epoch [227/300], Step [400/782], Loss: 1.4924\n",
            "Epoch [227/300], Step [500/782], Loss: 1.4924\n",
            "Epoch [227/300], Step [600/782], Loss: 1.4763\n",
            "Epoch [227/300], Step [700/782], Loss: 1.4765\n",
            "Epoch [228/300], Step [100/782], Loss: 1.4614\n",
            "Epoch [228/300], Step [200/782], Loss: 1.4768\n",
            "Epoch [228/300], Step [300/782], Loss: 1.4613\n",
            "Epoch [228/300], Step [400/782], Loss: 1.4615\n",
            "Epoch [228/300], Step [500/782], Loss: 1.4618\n",
            "Epoch [228/300], Step [600/782], Loss: 1.4615\n",
            "Epoch [228/300], Step [700/782], Loss: 1.4923\n",
            "Epoch [229/300], Step [100/782], Loss: 1.4612\n",
            "Epoch [229/300], Step [200/782], Loss: 1.4769\n",
            "Epoch [229/300], Step [300/782], Loss: 1.4768\n",
            "Epoch [229/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [229/300], Step [500/782], Loss: 1.4917\n",
            "Epoch [229/300], Step [600/782], Loss: 1.4762\n",
            "Epoch [229/300], Step [700/782], Loss: 1.4923\n",
            "Epoch [230/300], Step [100/782], Loss: 1.4844\n",
            "Epoch [230/300], Step [200/782], Loss: 1.4926\n",
            "Epoch [230/300], Step [300/782], Loss: 1.4768\n",
            "Epoch [230/300], Step [400/782], Loss: 1.4783\n",
            "Epoch [230/300], Step [500/782], Loss: 1.4932\n",
            "Epoch [230/300], Step [600/782], Loss: 1.4772\n",
            "Epoch [230/300], Step [700/782], Loss: 1.4768\n",
            "Epoch [231/300], Step [100/782], Loss: 1.4783\n",
            "Epoch [231/300], Step [200/782], Loss: 1.4918\n",
            "Epoch [231/300], Step [300/782], Loss: 1.4615\n",
            "Epoch [231/300], Step [400/782], Loss: 1.5077\n",
            "Epoch [231/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [231/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [231/300], Step [700/782], Loss: 1.4761\n",
            "Epoch [232/300], Step [100/782], Loss: 1.4768\n",
            "Epoch [232/300], Step [200/782], Loss: 1.5540\n",
            "Epoch [232/300], Step [300/782], Loss: 1.4680\n",
            "Epoch [232/300], Step [400/782], Loss: 1.4617\n",
            "Epoch [232/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [232/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [232/300], Step [700/782], Loss: 1.4929\n",
            "Epoch [233/300], Step [100/782], Loss: 1.4664\n",
            "Epoch [233/300], Step [200/782], Loss: 1.4960\n",
            "Epoch [233/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [233/300], Step [400/782], Loss: 1.4768\n",
            "Epoch [233/300], Step [500/782], Loss: 1.4924\n",
            "Epoch [233/300], Step [600/782], Loss: 1.4614\n",
            "Epoch [233/300], Step [700/782], Loss: 1.4614\n",
            "Epoch [234/300], Step [100/782], Loss: 1.4613\n",
            "Epoch [234/300], Step [200/782], Loss: 1.4770\n",
            "Epoch [234/300], Step [300/782], Loss: 1.4613\n",
            "Epoch [234/300], Step [400/782], Loss: 1.4613\n",
            "Epoch [234/300], Step [500/782], Loss: 1.4763\n",
            "Epoch [234/300], Step [600/782], Loss: 1.4613\n",
            "Epoch [234/300], Step [700/782], Loss: 1.4937\n",
            "Epoch [235/300], Step [100/782], Loss: 1.4771\n",
            "Epoch [235/300], Step [200/782], Loss: 1.4803\n",
            "Epoch [235/300], Step [300/782], Loss: 1.4792\n",
            "Epoch [235/300], Step [400/782], Loss: 1.4769\n",
            "Epoch [235/300], Step [500/782], Loss: 1.4612\n",
            "Epoch [235/300], Step [600/782], Loss: 1.5120\n",
            "Epoch [235/300], Step [700/782], Loss: 1.4924\n",
            "Epoch [236/300], Step [100/782], Loss: 1.4922\n",
            "Epoch [236/300], Step [200/782], Loss: 1.5233\n",
            "Epoch [236/300], Step [300/782], Loss: 1.4762\n",
            "Epoch [236/300], Step [400/782], Loss: 1.4930\n",
            "Epoch [236/300], Step [500/782], Loss: 1.4631\n",
            "Epoch [236/300], Step [600/782], Loss: 1.5083\n",
            "Epoch [236/300], Step [700/782], Loss: 1.4641\n",
            "Epoch [237/300], Step [100/782], Loss: 1.4616\n",
            "Epoch [237/300], Step [200/782], Loss: 1.4804\n",
            "Epoch [237/300], Step [300/782], Loss: 1.4764\n",
            "Epoch [237/300], Step [400/782], Loss: 1.4627\n",
            "Epoch [237/300], Step [500/782], Loss: 1.4618\n",
            "Epoch [237/300], Step [600/782], Loss: 1.4872\n",
            "Epoch [237/300], Step [700/782], Loss: 1.4761\n",
            "Epoch [238/300], Step [100/782], Loss: 1.4613\n",
            "Epoch [238/300], Step [200/782], Loss: 1.4766\n",
            "Epoch [238/300], Step [300/782], Loss: 1.4612\n",
            "Epoch [238/300], Step [400/782], Loss: 1.4766\n",
            "Epoch [238/300], Step [500/782], Loss: 1.4769\n",
            "Epoch [238/300], Step [600/782], Loss: 1.4769\n",
            "Epoch [238/300], Step [700/782], Loss: 1.4612\n",
            "Epoch [239/300], Step [100/782], Loss: 1.4922\n",
            "Epoch [239/300], Step [200/782], Loss: 1.4923\n",
            "Epoch [239/300], Step [300/782], Loss: 1.4781\n",
            "Epoch [239/300], Step [400/782], Loss: 1.5211\n",
            "Epoch [239/300], Step [500/782], Loss: 1.4764\n",
            "Epoch [239/300], Step [600/782], Loss: 1.4612\n",
            "Epoch [239/300], Step [700/782], Loss: 1.4926\n",
            "Epoch [240/300], Step [100/782], Loss: 1.4928\n",
            "Epoch [240/300], Step [200/782], Loss: 1.4612\n",
            "Epoch [240/300], Step [300/782], Loss: 1.4616\n",
            "Epoch [240/300], Step [400/782], Loss: 1.4781\n",
            "Epoch [240/300], Step [500/782], Loss: 1.4826\n",
            "Epoch [240/300], Step [600/782], Loss: 1.4662\n",
            "Epoch [240/300], Step [700/782], Loss: 1.5077\n",
            "Epoch [241/300], Step [100/782], Loss: 1.5081\n",
            "Epoch [241/300], Step [200/782], Loss: 1.4613\n",
            "Epoch [241/300], Step [300/782], Loss: 1.4627\n",
            "Epoch [241/300], Step [400/782], Loss: 1.4764\n",
            "Epoch [241/300], Step [500/782], Loss: 1.4614\n",
            "Epoch [241/300], Step [600/782], Loss: 1.4771\n",
            "Epoch [241/300], Step [700/782], Loss: 1.4629\n",
            "Epoch [242/300], Step [100/782], Loss: 1.4765\n",
            "Epoch [242/300], Step [200/782], Loss: 1.4765\n",
            "Epoch [242/300], Step [300/782], Loss: 1.4620\n",
            "Epoch [242/300], Step [400/782], Loss: 1.4612\n",
            "Epoch [242/300], Step [500/782], Loss: 1.4622\n",
            "Epoch [242/300], Step [600/782], Loss: 1.4614\n",
            "Epoch [242/300], Step [700/782], Loss: 1.4920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlXyy6C7mQjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f978dab-db23-4904-a470-f90615933a2c"
      },
      "source": [
        "# Test the model\n",
        "\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 40.24 %\n",
            "time: 2.26 s (started: 2021-10-15 17:19:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17bWQe1M-AKK",
        "outputId": "06cf3377-23ce-4498-a6ce-4c3bfab504e2"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for (inputs, labels) in test_loader:\n",
        "        images = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 40 %\n",
            "time: 2.2 s (started: 2021-10-15 17:20:46 +00:00)\n"
          ]
        }
      ]
    }
  ]
}